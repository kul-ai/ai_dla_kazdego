{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1189a75b",
   "metadata": {},
   "source": [
    "# Uczenie głębokie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5781386",
   "metadata": {},
   "source": [
    "### Uczenie głębokie a sztuczna inteligencja\n",
    "\n",
    "<img src=\"../pliki_wlasne/uczenie-glebokie.png\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928dc1e5",
   "metadata": {},
   "source": [
    "### Tensory\n",
    "\n",
    "<span t=\"l1\">Tensor - wielowymiarowa tablica:</span>\n",
    "- <span t=\"l2\">Tensor 0D - skalar.</span>\n",
    "- <span t=\"l2\">Tensor 1D - wektor.</span>\n",
    "- <span t=\"l2\">Tensor 2D - macierz.</span>\n",
    "- <span t=\"l2\">Tensor 3D.</span>\n",
    "- <span t=\"l2\">Tensor więcej wymiarowy.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69395d0",
   "metadata": {},
   "source": [
    "### Wybrane architektury uczenia głębokiego\n",
    "\n",
    "<span t=\"l1\">Wybrane architektury uczenia głębokiego oparte o sieci neuronowe:</span>\n",
    "- <span t=\"l2\">Głębokie sieci neuronowe (DNN – Deep Neural Networks).</span>\n",
    "- <span t=\"l2\">Rekurencyjne sieci neuronowe (RNN – Recurrent Neural Networks).</span>\n",
    "- <span t=\"l2\">Splotowe (konwolucyjne) sieci neuronowe (CNN – Convolutional Neural Networks).</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc06fb",
   "metadata": {},
   "source": [
    "<span t=\"t\">\n",
    "    \n",
    "| **sieć** \t|     **główna zaleta**     \t|\n",
    "|:--------:\t|:-------------------------:\t|\n",
    "|    DNN   \t| uczenie się hierarchiczne \t|\n",
    "|    RNN   \t|   uczenie się sekwencji   \t|\n",
    "|    CNN   \t|    redukcja liczby wag    \t|\n",
    "    \n",
    "</span>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf96947",
   "metadata": {},
   "source": [
    "### Głębokie sieci neuronowe\n",
    "\n",
    "<span t=\"l1\">Głębokie sieci neuronowe – sieci neuronowe z wieloma warstwami pomiędzy warstwą wejściową a warstwą wyjściową.</span>\n",
    "\n",
    "<img src=\"../pliki_wlasne/dnn.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37982216",
   "metadata": {},
   "source": [
    "### Rekurencyjne sieci neuronowe\n",
    "\n",
    "<span t=\"l1\">Rekurencyjne sieci neuronowe – sieci neuronowe ze sprzężeniami zwrotnymi.</span>\n",
    "\n",
    "<span t=\"l1\">Wybrane architektury:</span>\n",
    "\n",
    "\n",
    "- <span t=\"l2\">sieci ze sprzężeniami zwrotnymi dla neuronów ukrytych, generujące wartość wyjściową dla każdej chwili czasowej,</span>\n",
    "\n",
    "- <span t=\"l2\">sieci ze sprzężeniami zwrotnymi pomiędzy neuronami wyjściowymi a neuronami ukrytymi, generujące wartość wyjściową dla każdej chwili czasowej,</span>\n",
    "\n",
    "- <span t=\"l2\">sieci ze sprzężeniami zwrotnymi dla neuronów ukrytych, generujące wartość wyjściową po przetworzeniu całej sekwencji.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946048e4",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Sieci ze sprzężeniami zwrotnymi dla neuronów ukrytych, generujące wartość wyjściową dla każdej chwili czasowej</span>\n",
    "\n",
    "<span t=\"l1\">Graf obliczeń w architekturze rekurencyjnej sieci neuronowej (przykład 1):</span>\n",
    "\n",
    "\n",
    "<img src=\"../pliki_wlasne/rnn1.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "<span t=\"l1\">Źródło: Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning: systemy uczące się. PWN, Warszawa, 2018.</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fd9c8",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Sieci ze sprzężeniami zwrotnymi pomiędzy neuronami wyjściowymi a neuronami ukrytymi, generujące wartość wyjściową dla każdej chwili czasowej</span>\n",
    "\n",
    "<span t=\"l1\">Graf obliczeń w architekturze rekurencyjnej sieci neuronowej (przykład 2):</span>\n",
    "\n",
    "\n",
    "<img src=\"../pliki_wlasne/rnn2.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "<span t=\"l1\">Źródło: Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning: systemy uczące się. PWN, Warszawa, 2018.</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0fa5ba",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Sieci ze sprzężeniami zwrotnymi dla neuronów ukrytych, generujące wartość wyjściową po przetworzeniu całej sekwencji.</span>\n",
    "\n",
    "<span t=\"l1\">Graf obliczeń w architekturze rekurencyjnej sieci neuronowej (przykład 3):</span>\n",
    "\n",
    "\n",
    "<img src=\"../pliki_wlasne/rnn3.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "<span t=\"l1\">Źródło: Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning: systemy uczące się. PWN, Warszawa, 2018.</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68a4bf",
   "metadata": {},
   "source": [
    "### Splotowe sieci neuronowe\n",
    "\n",
    "<span t=\"l1\">Splotowe sieci neuronowe – sieci neuronowe, które w co najmniej jednej warstwie zamiast mnożenia macierzy wykorzystują operację splotu.</span>\n",
    "\n",
    "<span t=\"l1\">Splot ciągów dyskretnych $f(n)$ i $g(n)$ zdefiniowanych dla całkowitych wartości $n$:</span>\n",
    "\n",
    "<span t=\"l2\">$(f*g)(i)=\\sum_{j=-\\propto}^{\\propto } f(j)g(i-j)= \\sum_{j=-\\propto }^{\\propto }f(i-j)g(j)$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de694bb",
   "metadata": {},
   "source": [
    "<span t=\"l1\">$s(i)=(f*g)(i)$</span>\n",
    "\n",
    "<span t=\"l1\">W przypadku splotowych sieci neuronowych przyjmuje się często następujące nazewnictwo:</span>\n",
    "- <span t=\"l2\">$f$ - wejście</span>\n",
    "- <span t=\"l2\">$g$ - jądro (filtr)</span>\n",
    "- <span t=\"l2\">$s$ - odwzorowanie cech</span>\n",
    "\n",
    "<span t=\"l1\">Zwykle:</span>\n",
    "\n",
    "- <span t=\"l2\">Wejście – wielowymiarowa tablica (tensor) danych.</span>\n",
    "- <span t=\"l2\">Jądro (filtr) – wielowymiarowa tablica (tensor) parametrów.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53440561",
   "metadata": {},
   "source": [
    "<span t=\"l1\">W przypadku dwuwymiarowego wejścia (np. obraz) i dwuwymiarowego jądra (filtru):</span>\n",
    "\n",
    "<span t=\"l2\">$(f*g)(i,j)=\\sum_{k=-\\propto}^{\\propto} \\sum_{l=-\\propto}^{\\propto}f(k,l)g(i-k,j-l)$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac487dc",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Przykład operacji splotu</span>\n",
    "\n",
    "- <span t=\"l1\">wejście:</span>\n",
    "\n",
    "<span t=\"l2\">\n",
    "\\begin{bmatrix}\n",
    "a & b & c & d\\\\\n",
    "e & f & g & h\\\\\n",
    "i & j & k & l\n",
    "\\end{bmatrix}\n",
    "</span>\n",
    "\n",
    "- <span t=\"l1\">jądro: </span>\n",
    "\n",
    "<span t=\"l2\">\n",
    "\\begin{bmatrix}\n",
    "w & x\\\\\n",
    "y & z\n",
    "\\end{bmatrix} \n",
    " </span>\n",
    "\n",
    "- <span t=\"l1\">wynik:</span>\n",
    "<span t=\"l2\">\n",
    "    \n",
    "\\begin{bmatrix}\n",
    "aw+bx+ey+fz & bw+cx+fy+gz & cw+dx+gy+hz \\\\ \n",
    "ew+fx+iy+jz & fw+gx+jy+kz & gw+hx+ky+lz  \n",
    "\\end{bmatrix} \n",
    "</span>\n",
    "\n",
    "<span t=\"l1\">Źródło: Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning: systemy uczące się. PWN, Warszawa, 2018.</span>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71499d",
   "metadata": {},
   "source": [
    "### Operacja splotu dwuwymiarowego\n",
    "\n",
    "<img src=\"../pliki_wlasne/Splot_1.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bb93d",
   "metadata": {},
   "source": [
    "### Operacja splotu dwuwymiarowego\n",
    "\n",
    "<img src=\"../pliki_wlasne/Splot_2.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c2cd3",
   "metadata": {},
   "source": [
    "### Zastosowanie konwolucyjnych sieci neuronowych\n",
    "\n",
    "<span t=\"l1\">Konwolucyjne sieci neuronowe wykorzystywane są w przetwarzaniu obrazów.</span>\n",
    "\n",
    "- <span t=\"l2\"> W procesie uczenia każdy neuron w warstwie konwolucyjnej powinien wykrywać inne szczegóły obrazu.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9bb5e",
   "metadata": {},
   "source": [
    "### Warstwa gęsta (dense layer)\n",
    "\n",
    "<span t=\"l1\">Każdy neuron jest połączony z każdym wyjściem neuronów poprzedzających.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a4e4b",
   "metadata": {},
   "source": [
    "### Warstwa LSTM \n",
    "\n",
    "<span t=\"l1\">LSTM (Long Short-Term Memory layer) – długa pamięć krótkotrwała.</span>\n",
    "\n",
    "<span t=\"l2\">Komórka LSTM:</span>\n",
    "\n",
    "<img src=\"../pliki_z_internetu/lstm.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4e05d",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Obliczenia dla komórki LSTM:</span>\n",
    "\n",
    "- <span t=\"l2\">$f_t=\\sigma_g(W_f x_t+U_f c_{t-1}+b_f)$ </span>\n",
    "- <span t=\"l2\">$i_t=\\sigma_g(W_i x_t+U_i c_{t-1}+b_i)$</span>\n",
    "- <span t=\"l2\">$o_t=\\sigma_g(W_o x_t+U_o c_{t-1}+b_o)$</span>\n",
    "- <span t=\"l2\">$c_t=f_t circ c_{t-1}+i_t \\circ \\sigma_h(W_c x_t + b_c)$</span>\n",
    "- <span t=\"l2\">$h_t= \\sigma_h(o_t \\circ c_t)$</span>\n",
    "\n",
    "<span t=\"l1\">Gdzie:</span>\n",
    "- <span t=\"l2\">$\\sigma_g$ - funkcja sigmoidalna </span>\n",
    "- <span t=\"l2\">$\\sigma_h$ - tangens hiperboliczny </span>\n",
    "- <span t=\"l2\">$\\circ$ - iloczyn Hadamarda</span>\n",
    "\n",
    "<span t=\"l1\">Główna zaleta warstwy LSTM to łatwość długiego zapamiętywania krótkich wzorców.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
