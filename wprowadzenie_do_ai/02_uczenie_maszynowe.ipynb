{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7884b1d",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653549e",
   "metadata": {},
   "source": [
    "## Uczenie maszynowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e7e97",
   "metadata": {},
   "source": [
    "### Systemy uczące się\n",
    "\n",
    "- <span t=\"l1\">Systemy uczące się to algorytmiczne metody uczenia się na podstawie danych.</span>\n",
    "- <span t=\"l1\">Maszynowe uczenie się obejmuje problematykę konstruowania systemów (programów) komputerowych, </span>które polepszają swoje działanie wraz z analizą doświadczenia reprezentowanego przez zbiór przykładów uczących.\n",
    "- <span t=\"l1\">System uczący może oprócz przykładów uczących wykorzystywać wiedzę dziedzinową.</span>\n",
    "- <span t=\"l1\">Metody uczenia się:</span>\n",
    "    - <span t=\"l2\">uczenie nadzorowane (z nauczycielem), </span>\n",
    "    - <span t=\"l2\">uczenie nienadzorowane (bez nauczyciela).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322f98e",
   "metadata": {},
   "source": [
    "### Uczenie maszynowe\n",
    "\n",
    "<img src=\"../pliki_wlasne/uczenie-maszynowe.png\" width=\"500\"/>\n",
    "\n",
    "- Uczenie maszynowe (machine learning)\n",
    "    - Klasteryzacja (grupowanie) danych\n",
    "    - Klasyfikacja danych\n",
    "    - Odkrywanie wiedzy z danych\n",
    "    - Regresja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4e1ed",
   "metadata": {},
   "source": [
    "<img src=\"../pliki_wlasne/uczenie-maszynowe2.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "- Uczenie maszynowe\n",
    "    - Identyfikacja cech (atrybutów, właściwości) przypadków (obiektów)\n",
    "    - Wstępne przetwarzanie danych\n",
    "    - Algorytm uczenia maszynowego (ew. budowa modelu)\n",
    "    - Wizualizacja wyników"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28348177",
   "metadata": {},
   "source": [
    "### Wstępne przetwarzanie danych\n",
    "\n",
    "<span t=\"l1\">Generalna zasada</span>\n",
    "> <span t=\"q\">Śmieci na wyjściu - śmieci na wejściu</span>\n",
    "\n",
    "<img src=\"../pliki_wlasne/gigo.png\" width=\"500\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d87791",
   "metadata": {},
   "source": [
    "- <span t=\"l1\">Integracja danych</span>\n",
    "- <span t=\"l1\">Filtrowanie danych (m.in. problem selekcji istotnych cech)</span>\n",
    "- <span t=\"l1\">Czyszczenie danych (m.in. problem brakujących danych)</span>\n",
    "- <span t=\"l1\">Transformacja danych (m.in. problemy: różnych typów danych, brakujących danych)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fd39a",
   "metadata": {},
   "source": [
    "### Klasteryzacja\n",
    "\n",
    "<span t=\"l1\">Przypadki - Algorytm uczenia maszynowego - Przypadki pogrupowane</span>\n",
    "\n",
    "<img src=\"../pliki_wlasne/klasteryzacja.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b111ed",
   "metadata": {},
   "source": [
    "### Klasyfikacja\n",
    "\n",
    "\n",
    "- <span t=\"l1\">Uczenie się klasyfikatora ze zbioru treningowego</span>\n",
    "    - <span t=\"l2\">Zbiór przypadków treningowych (ze znanymi decyzjami klasyfikującymi) - Klasyfikator (budowany za pomocą algorytmu uczenia maszynowego)</span>\n",
    "    \n",
    "    <img src=\"../pliki_wlasne/klasyfikacja1.png\" width=\"500\"/>\n",
    "\n",
    "- <span t=\"l1\">Klasyfikacja nowych przypadków</span>\n",
    "    - <span t=\"l2\">Nowe przypadki - Klasyfikator - Decyzje klasyfikacyjne dla nowych przypadków</span>\n",
    " \n",
    "    <img src=\"../pliki_wlasne/klasyfikacja2.png\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13da3fc",
   "metadata": {},
   "source": [
    "### Odkrywanie wiedzy z danych\n",
    "\n",
    "- <span t=\"l1\">Odkrywanie reguł asocjacyjnych</span>\n",
    "    - <span t=\"l2\">systemy rekomendujące</span>\n",
    "- <span t=\"l1\">Przykład: analiza koszyka</span>\n",
    "    - <span t=\"l2\">Czy jabłka były kupowane wtedy gdy kupowano banany i pomarańcze?</span>\n",
    "    - <span t=\"l2\">Czy gruszki były kupowane równocześnie z wiśniami?</span>\n",
    "    - <span t=\"l2\">Czy ... ?</span>\n",
    "    \n",
    "    <img src=\"../pliki_wlasne/analiza-koszyka.png\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4a23b",
   "metadata": {},
   "source": [
    "- <span t=\"l1\">Odkrywanie wzorców (np. w danych czasowych)</span>\n",
    "    - <span t=\"l2\">wzorce często występujące </span>\n",
    "    - <span t=\"l2\">wzorce nietypowe - anomalie</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b414d07",
   "metadata": {},
   "source": [
    "### Regresja\n",
    "- <span t=\"l1\">Predykcja – obliczanie brakujących wartości w zakresie wartości danych treningowych</span>\n",
    "- <span t=\"l1\">Ekstrapolacja – obliczanie brakujących wartości poza zakresem wartości danych treningowych (np. przyszłych wartości)</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e536ee3",
   "metadata": {},
   "source": [
    "## Klasteryzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d02b77",
   "metadata": {},
   "source": [
    "### Klasteryzacja – podstawowe założenia\n",
    "\n",
    "<span t=\"l1\">Podstawowym zadaniem klasteryzacji (grupowania) jest dokonanie podziału zbioru przypadków $C$ znajdujących się w bazie na grupy $C_1, C_2, ..., C_k$, nazywane klastrami, stanowiące podzbiory przypadków podobnych do siebie, przy czym pojęcie podobieństwa może być definiowane w różny sposób.</span>\n",
    "\n",
    "<span t=\"l1\">Podział zbioru $C$ powinien być dokonany w taki sposób, aby przypadki z danej grupy były bardziej podobne do siebie (homogeniczność) niż do jakichkolwiek przypadków z pozostałych grup (heterogeniczność).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1832efb",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Istotnym zagadnieniem jest ustalenie liczby $k$ grup, na które zbiór przypadków ma zostać podzielony, gdyż zazwyczaj liczba ta nie jest z góry zadana.</span>\n",
    "\n",
    "<span t=\"l1\">Kryteria klasteryzacji dotyczą interpretacji semantycznej klastrów.</span>\n",
    "\n",
    "<span t=\"l1\">Istotna jest odpowiedź na pytanie dlaczego dwa przypadki przypisywane są do tego samego klastra. W tej kwestii odpowiedź może być udzielona na podstawie dostępnej wiedzy.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8aaf4",
   "metadata": {},
   "source": [
    "<span t=\"l1\">W wielu sytuacjach przypadki grupowane są razem ze względu na istniejące pomiędzy nimi zależności takie jak np. noeodróżnialność, podobieństwo, bliskość, funkcjonalność, zgodność.</span>\n",
    "\n",
    "<span t=\"l1\">Istotnym zagadnieniem w procesie klasteryzacji zbioru przypadków jest ustalenie struktury klastrów.</span>\n",
    "\n",
    "<span t=\"l1\">Klastry mogą być parami rozłączne lub też zachodzące na siebie.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd06ed",
   "metadata": {},
   "source": [
    "<span t=\"l1\">W przypadku klastrów rozłącznych mówi się o tzw. podziale ostrym. W takiej sytuacji dany przypadek należy tylko do jednego klastra.</span>\n",
    "\n",
    "<span t=\"l1\">W przypadku klastrów zachodzących na siebie mówi się o tzw. podziale rozmytym. Przy tym podziale dany przypadek może należeć do wielu klastrów.</span>\n",
    "\n",
    "<span t=\"l1\">Dodatkowo określany jest stopień przynależności przypadku do danego klastra.</span>\n",
    "\n",
    "<span t=\"l1\">Stopień ten ma wartość rozmytą z przedziału $[0, 1]$. Wynika z tego, że przypadek może należeć do grupy tylko w pewnym stopniu.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20815a46",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Dla podziałów rozmytych możliwe są dwie sytuacje:</span>\n",
    "- <span t=\"l2\">W pierwszej z nich suma stopni przynależności danego przypadku do każdego z klastrów jest zawsze równa 1 (tzw. podział probabilistyczny).</span>\n",
    "- <span t=\"l2\">W drugiej sytuacji warunek sumowania się stopni przynależności do 1 nie obowiązuje (tzw. podział posybilistyczny).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156a955",
   "metadata": {},
   "source": [
    "### Miary odległości\n",
    "\n",
    "<span t=\"l1\">Dane są dwa punkty x oraz y w przestrzeni m- wymiarowej:</span>\n",
    "\n",
    "<span t=\"l2\">$x=[x_1, x_2, ...,x_m]$ </span>\n",
    "\n",
    "<span t=\"l2\">$y=[y_1, y_2 ,..., y_m]$</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbec23",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Odległość (metryka) Euklidesa</span>\n",
    "\n",
    "<span t=\"l2\">$ d_{Eukl}(x,y)=\\sqrt{\\sum_{i=1}^{n} (y_i-x_i)^2  } $</span>\n",
    "\n",
    "<span t=\"l1\">Odległość (metryka) Manhattanu</span>\n",
    "\n",
    "<span t=\"l2\">$d_{Manh}(x,y)=\\sum_{i=1}^{n} | y_i-x_i |$</span>\n",
    "\n",
    "<span t=\"l1\">Odległość (metryka) Minkowskiego</span>\n",
    "\n",
    "<span t=\"l2\">$d_{Mink}(x,y)=\\sqrt[q]{\\sum_{i=1}^{n} | y_i-x_i |^q} $</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b6ee2",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Dla atrybutów symbolicznych możemy zdefiniować funkcję R („różne od”).</span>\n",
    "\n",
    "<span t=\"l1\">Dla $i$-tego atrybutu funkcja ma postać: </span>\n",
    "\n",
    "<span t=\"l2\">\n",
    "$R(x_i,y_i)=\\left\\{\n",
    "\\begin{array}{ccc}\n",
    "0&\\mbox{dla}&x_i=y_i\\\\\n",
    "1&\\mbox{}&\\mbox{w przeciwnym przypadku}\n",
    "\\end{array}\n",
    "\\right.$</span>\n",
    "\n",
    "<span t=\"l1\">Funkcja $R$ może zostać zastosowana dla i-tego atrubutu w miarach odległości.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a84854",
   "metadata": {},
   "source": [
    "### Normalizacja wartości atrybutów\n",
    "\n",
    "<span t=\"l1\">Przy wyznaczaniu odległości atrybuty posiadające duże wartości mogą niwelować wpływ innych atrybutów (tych posiadających mniejsze wartości).</span>\n",
    "\n",
    "<span t=\"l1\">W celu wyeliminowania tej sytuacji należy dokonać normalizacji wartości atrybutów.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159e943",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Normalizacja min-max</span>\n",
    "\n",
    "<span t=\"l2\">Dla zbioru wartości atrybutu: $X=\\{x_1,x_2,...,x_k\\}$</span>\n",
    "\n",
    "<span t=\"l2\">Znormalizowanawartość $x_i$ jest obliczana jako: $x_i^{*}=\\frac{x_i- min (X)}{zakres(X)}= \\frac{x_i- min (X)}{max(X)-min(X)}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dd95f",
   "metadata": {},
   "source": [
    "### Algorytmy iteracyjnej optymalizacji\n",
    "\n",
    "<span t=\"l1\">W algorytmach iteracyjnej optymalizacji najlepszy podział zbioru przypadków jest wyznaczany przez iteracyjne polepszanie pewnych wskaźników jakości, startując z początkowego, najczęściej losowego, podziału.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7885d163",
   "metadata": {},
   "source": [
    "### Algorytm HCM (Hard C-Means)\n",
    "\n",
    "<span t=\"l1\">Algorytm dzieli jednoznacznie zbiór przykładów na $c$ grup.</span>\n",
    "\n",
    "<span t=\"l1\">Środek grupy jest średnią położenia wszystkich przykładów należących do tej grupy.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a95ae",
   "metadata": {},
   "source": [
    "- <span t=\"l1\">KROK 1: Ustalenie liczby (c) grup, na które zbiór przypadków będzie dzielony.</span>\n",
    "- <span t=\"l1\">KROK 2: Losowe przypisanie c rekordów jako początkowych środków grup.</span>\n",
    "- <span t=\"l1\">KROK 3: Znalezienie dla każdego rekordu najbliższego środka grupy.</span>\n",
    "- <span t=\"l1\">KROK 4: Dla każdej z c grup znalezienie centroidu grupy i uaktualnienie położenie każdego środka grupy jako nową wartość centroidu.</span>\n",
    "- <span t=\"l1\">KROK 5: Powtórzenie kroków od 3 do 5 aż do osiągnięcia warunku zakończenia algorytmu.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d97617",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Obliczenie centroidu grupy dla grupy przypadków.</span>\n",
    "\n",
    "<span t=\"l1\">Dla grupy przypadków</span>\n",
    "\n",
    "<span t=\"l2\">$x^1=[x_1^1, x_2^1, ..., x_m^1]$</span>\n",
    "\n",
    "<span t=\"l2\">$x^2=[x_1^2, x_2^2, ..., x_m^2]$</span>\n",
    "\n",
    "<span t=\"l2\">$...$</span>\n",
    "\n",
    "<span t=\"l2\">$x^k=[x_1^k, x_2^k, ..., x_m^k]$</span>\n",
    "\n",
    "\n",
    "<span t=\"l1\">centroid ma postać:</span>\n",
    "<span t=\"l2\">$(\\frac{\\sum_{i=1}^{k} x_1^i}{k}, \\frac{\\sum_{i=1}^{k} x_2^i}{k}, ...,  \\frac{\\sum _{i=1}^{k} x_m^i}{k})$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a333ce4",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Warunkiem zakończenia algorytmu może być sytuacja gdy dla wszystkich grup wszystkie przypadki przypisane do środka tej grupy pozostają w tej grupie.</span>\n",
    "\n",
    "<span t=\"l1\">Inaczej, algorytm kończy działanie gdy osiągnięte zostanie kryterium zbieżności – minimalizacja sumarycznego błędu kwadratowego ($p$ jest przykładem z $i$-tej grupy, $m_i$ jest centroidem i-tej grupy):</span>\n",
    "\n",
    "<span t=\"l2\">$SSE=\\sum_{i=1}^{c}\\sum_{p \\in C_i}^{} d(p,m_i)^2$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9427d",
   "metadata": {},
   "source": [
    "### Algorytm FCM (Fuzzy C-Means)\n",
    "\n",
    "<span t=\"l1\">Algorytm dzieli zbiór przykładów na $c$ grup. </span>\n",
    "\n",
    "<span t=\"l1\">Przykłady mogą należeć do różnych grup z odpowiednimi stopniami przynależności.</span>\n",
    "\n",
    "<span t=\"l1\">Suma przynależności danego przykładu do każdej z grup jest zawsze równa 1.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdf2c8",
   "metadata": {},
   "source": [
    "### Algorytm PCM (Possibilistic C-Means)\n",
    "\n",
    "<span t=\"l1\">Algorytm dzieli zbiór przykładów na c grup. </span>\n",
    "\n",
    "<span t=\"l1\">Przykłady mogą należeć do różnych grup z odpowiednimi stopniami przynależności.</span>\n",
    "\n",
    "<span t=\"l1\">Suma przynależności danego przykładu do każdej z grup nie musi być równa 1.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4983f",
   "metadata": {},
   "source": [
    "### Klasteryzacja hierarchiczna\n",
    "\n",
    "<span t=\"l1\">W klasteryzacji hierarchicznej danych tworzona jest struktura drzewiasta (dendrogram) poprzez rekurencyjne dzielenie lub łączenie istniejących grup.</span>\n",
    "\n",
    "<span t=\"l1\">Metody aglomeracyjne:</span>\n",
    "\n",
    "- <span t=\"l2\">na początku zakłada się, że każdy przykład stanowi oddzielną grupę,</span>\n",
    "- <span t=\"l2\">w kolejnych krokach dwie grupy, które są najbliżej siebie, łączy się w nową wspólną grupę,</span>\n",
    "- <span t=\"l2\">ostatecznie wszystkie przykłady należą do jednej grupy.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e52c2",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Metody rozdzielające:</span>\n",
    "\n",
    "- <span t=\"l2\">na początku zakłada się, że wszystkie przykłady należą do jednej grupy,</span>\n",
    "- <span t=\"l2\">w kolejnych krokach najbardziej niepodobne przykłady rozdzielane są w osobne grupy, </span>\n",
    "- <span t=\"l2\">ostatecznie każdy przykład stanowi oddzielną grupę.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee740d7f",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Kryteria określania odległości pomiędzy grupami:</span>\n",
    "- <span t=\"l2\">metoda pojedynczego połączenia (metoda najbliższego sąsiedztwa) – określana jest minimalna odległość pomiędzy dwoma przykładami należącymi do różnych grup,</span>\n",
    "- <span t=\"l2\">metoda całkowitego połączenia (metoda najdalszego sąsiedztwa) – określana jest maksymalna odległość pomiędzy dwoma przykładami należącymi do różnych grup,</span>\n",
    "- <span t=\"l2\">metoda średniego połączenia – określana jest średnia odległość pomiędzy wszystkimi przykładami z jednej grupy i wszystkimi przykładami z drugiej grupy.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ca6f0",
   "metadata": {},
   "source": [
    "## Klasyfikacja danych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b97ee6",
   "metadata": {},
   "source": [
    "### Tabela decyzyjna – przykład\n",
    "\n",
    "\n",
    "<img src=\"../pliki_wlasne/tabela-decyzyjna.png\" width=\"500\"/>\n",
    "\n",
    "<span t=\"l1\">Źródło: D.T. Larose: „Odkrywanie wiedzy z danych”. PWN, Warszawa, 2006.</span>\n",
    "\n",
    "- <span t=\"l2\">Atrybuty warunkowe (opisujące przypadki) - Atrybuty decyzyjny</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d9051",
   "metadata": {},
   "source": [
    "###  Problem klasyfikacji\n",
    "\n",
    "\n",
    "<img src=\"../pliki_wlasne/problem-klasyfikacji.png\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3029e0",
   "metadata": {},
   "source": [
    "### Klasyfikatory\n",
    "\n",
    "<span t=\"l1\">Klasyfikatory oparte o model, np.:</span>\n",
    "- <span t=\"l2\">model w postaci sieci neuronowej </span>\n",
    "- <span t=\"l2\">model w postaci drzewa decyzyjnego </span>\n",
    "- <span t=\"l2\">model w postaci zbioru reguł (IF-THEN) </span>\n",
    "- <span t=\"l2\">model w postaci sieci Bayesa</span>\n",
    "\n",
    "<span t=\"l1\">Klasyfikatory leniwe, np.:</span>\n",
    "- <span t=\"l2\">k-NN (k najbliższych sąsiadów)</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27bb69",
   "metadata": {},
   "source": [
    "###  Ocena jakości klasyfikatora\n",
    "\n",
    "<img src=\"../pliki_wlasne/ocena-jakosci-klasyfikatora.png\" width=\"500\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87278c9d",
   "metadata": {},
   "source": [
    "#### <span t=\"l1\">Macierz pomyłek (konfuzji, rozrzutu)</span>\n",
    "\n",
    "<img src=\"../pliki_wlasne/macierz-pomylek.png\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42dd4da",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Strategie</span>\n",
    "- <span t=\"l2\">Niezależne zbiory: treningowy i testowy.</span>\n",
    "- <span t=\"l2\">Losowy podział zbioru wszystkich przypadków na zbiory: treningowy i testowy w ustalonej proporcji.</span>\n",
    "- <span t=\"l2\">Kroswalidacja k-krotna; kroswalidacja stratyfikowana k-krotna (zachowane są oryginalne proporcje pomiędzy klasami decyzyjnymi).</span>\n",
    "- <span t=\"l2\">Leave one out (szczególny przypadek kroswalidacji k-krotnej).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e3ffe",
   "metadata": {},
   "source": [
    "#### <span t=\"l1\">Kroswalidacja k-krotna</span>\n",
    "\n",
    "<span t=\"l1\">Podzbiory $[1..k]$ ← losowy podział zbioru przypadków na k podzbiorów</span>\n",
    "\n",
    "<span t=\"l1\"> Dla każdego $i$ od 1 do $k$:</span>\n",
    "- <span t=\"l2\">zbiór treningowy ← podzbiory z wyjątkiem i-tego </span>\n",
    "- <span t=\"l2\">zbiór testowy ← podzbiór i-ty</span>\n",
    "- <span t=\"l2\">uczenie klasyfikatora na zbiorze treningowym </span>\n",
    "- <span t=\"l2\">testowanie klasyfikatora na zbiorze testowym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e271648",
   "metadata": {},
   "source": [
    "#### <span t=\"l1\"> Leave one out (n - liczba przypadków)</span>\n",
    "\n",
    "Dla każdego $i$ od 1 do $n$:\n",
    "- <span t=\"l2\">zbiór treningowy ← zbiór przypadków z wyjątkiem $i$-tego</span>\n",
    "- <span t=\"l2\">zbiór testowy ← $i$-ty przypadek</span>\n",
    "- <span t=\"l2\">uczenie klasyfikatora na zbiorze treningowym</span>\n",
    "- <span t=\"l2\">testowanie klasyfikatora na zbiorze testowym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d8f6f",
   "metadata": {},
   "source": [
    "<span t=\"t\">\n",
    "    \n",
    "| Klasa przewidywana →<br>Klasa aktualna ↓ \t| Pozytywna \t| Negatywna \t|\n",
    "|:----------------------------------------:\t|:---------:\t|:---------:\t|\n",
    "|                 Pozytywna                \t|     TP    \t|     FN    \t|\n",
    "|                 Negatywna                \t|     FP    \t|     TN    \t|\n",
    "</span>\n",
    "\n",
    "- <span t=\"l1\">TP (true positive) – liczba przypadków prawdziwie pozytywnych</span>\n",
    "- <span t=\"l1\">FP (false positive) – liczba przypadków fałszywie pozytywnych</span>\n",
    "- <span t=\"l1\">TN (true negative) – liczba przypadków prawdziwie negatywnych</span>\n",
    "- <span t=\"l1\">FN (false negative) – liczba przypadków fałszywie negatywnych</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120bc4d",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Dokładność (accuracy):</span>\n",
    "\n",
    "<span t=\"l2\">$accuracy=\\frac{TP+TN}{TP+FP+TN+FN}$</span>\n",
    "\n",
    "<span t=\"l1\">Precyzja (precision):</span>\n",
    "\n",
    "<span t=\"l2\">$precision=\\frac{TP}{TP+FP}$</span>\n",
    "\n",
    "<span t=\"l1\">Zwrot (recall)</span>\n",
    "\n",
    "<span t=\"l2\">$recall=\\frac{TP}{TP+FN} $</span>\n",
    "\n",
    "<span t=\"l1\">F-miara (F-masure)</span>\n",
    "\n",
    "<span t=\"l2\">$F=\\frac{2 \\cdot precision cdot recall }{precision+recall}$</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4dee4c",
   "metadata": {},
   "source": [
    "### Drzewa decyzyjne\n",
    "\n",
    "<span t=\"l1\">Metody uczenia się drzew decyzyjnych to najbardziej znane i najczęściej stosowane w praktyce algorytmy indukcji symbolicznej reprezentacji wiedzy z przykładów.</span>\n",
    "\n",
    "<span t=\"l1\">Struktura drzewa jest dość czytelna dla człowieka.</span>\n",
    "\n",
    "<span t=\"l1\">Drzewo decyzyjne składa się z korzenia, z którego wychodzą co najmniej dwie gałęzie do węzłów leżących na niższym poziomie.</span>\n",
    "\n",
    "<span t=\"l1\">Z każdym węzłem związany jest test sprawdzający wartości atrybutu opisującego przykłady.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd45306",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Dla każdego z możliwych wyników testu odpowiadająca mu gałąź prowadzi do węzła na niższym poziomie drzewa.</span>\n",
    "\n",
    "<span t=\"l1\">Węzłom, z których nie wychodzą żadne gałęzie (nazywanym liśćmi) przypisane są odpowiednie klasy decyzyjne.</span>\n",
    "\n",
    "<span t=\"l1\">Ścieżki prowadzące od korzenia do liścia drzewa reprezentują koniunkcję pewnych testów zdefiniowanych na wartościach atrybutów opisujących przykłady uczące.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c06b31",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Drzewo decyzyjne może więc posłużyć do określenia zbioru reguł określających przydział przykładów do klas decyzyjnych.</span>\n",
    "\n",
    "<span t=\"l1\">Każda ścieżka drzewa od korzenia do liścia odpowiada jednej regule.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28418e",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Przykład</span>\n",
    "\n",
    "<img src=\"../pliki_wlasne/drzewo-decyzyjne.png\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e5921",
   "metadata": {},
   "source": [
    "### Konstruowanie drzew decyzyjnych\n",
    "\n",
    "<span t=\"l1\">Większość algorytmów konstrukcji (uczenia się) drzew decyzyjnych oparta jest na schemacie zstępującego konstruowania drzewa (TDIDT – Top Down Induction of Decision Trees).</span>\n",
    "\n",
    "<span t=\"l1\">Przykładowe algorytmy:</span>\n",
    "- <span t=\"l2\">CART </span>\n",
    "- <span t=\"l2\">ID3 </span>\n",
    "- <span t=\"l2\">C4.5</span>\n",
    "\n",
    "<span t=\"l1\">Generalnie różnice pomiędzy poszczególnymi algorytmami dotyczą wyboru optymalnego podziału.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f06f03b",
   "metadata": {},
   "source": [
    "### Algorytm C4.5\n",
    "\n",
    "<span t=\"l1\">Został zaproponowany przez Quinlana.</span>\n",
    "\n",
    "<span t=\"l1\">Przy wyborze optymalnego podziału algorytm wykorzystuje pojęcie zysku informacji.</span>\n",
    "\n",
    "<span t=\"l1\">Algorytm umożliwia generowanie drzewa decyzyjnego dla systemu z brakującymi wartościami atrybutów.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb876e",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Testy dla atrybutów symbolicznych</span>\n",
    "- <span t=\"l2\">dla każdej wartości atrybutu symbolicznego tworzona jest osobna gałąź\n",
    "\n",
    "<span t=\"l1\">Testy dla atrybutów numerycznych ciągłych; stosowany jest test binarny:</span>\n",
    "- <span t=\"l2\">wartość atrybutu mniejsza lub równa od wartości progowej</span>\n",
    "- <span t=\"l2\">wartość atrybutu większa od wartości progowej</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e719ca6",
   "metadata": {},
   "source": [
    "### Entropia zbioru treningowego\n",
    "\n",
    "<span t=\"l1\">$E(S)=- \\sum_{i=1}^{k} p_i log p_i$</span>\n",
    "\n",
    "- <span t=\"l2\">$k$ – liczba klas decyzyjnych</span>\n",
    "- <span t=\"l2\">$n$ – liczba wszystkich przykładów uczących (treningowych)</span>\n",
    "- <span t=\"l2\">$n_i$ – liczba przykładów uczących (treningowych) należących do $i$-tej klasy</span>\n",
    "- <span t=\"l2\">$p_i=\\frac{n_i}{n}$</span>\n",
    "- <span t=\"l2\">$i=1,2. ..., k$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07027080",
   "metadata": {},
   "source": [
    "### Zysk (przyrost) informacji\n",
    "\n",
    "<span t=\"l1\">$IG(S,a)=E(S) - \\sum_{v \\in V_{a}}^{} \\frac{n_{v}}{n} E(S_{v})$</span>\n",
    "\n",
    "- <span t=\"l2\">$a$ – cecha (atrybut)</span>\n",
    "- <span t=\"l2\">$V$ – zbiór wartości cechy $a$</span>\n",
    "- <span t=\"l2\">$n_v$ – liczba przykładów uczących (treningowych), dla których cecha $a$ ma wartość $v$</span>\n",
    "- <span t=\"l2\">$S_v$ – podzbiór zbioru przykładów uczących (treningowych), dla których cecha $a$ ma wartość $v$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d3128",
   "metadata": {},
   "source": [
    "### Możliwe podziały dla C4.5 – przykład\n",
    "\n",
    "<span t=\"l1\">Źródło: D.T. Larose: „Odkrywanie wiedzy z danych”. PWN, Warszawa, 2006.</span>\n",
    "\n",
    "<img src=\"../pliki_wlasne/c45-przyklad.png\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51477306",
   "metadata": {},
   "source": [
    "### Zjawisko przeuczenia\n",
    "\n",
    "<span t=\"l1\">Drzewo decyzyjne jest przeuczone (nadmiernie dopasowane) do danego zbioru uczącego, jeśli istnieje inne drzewo o większym błędzie na tym zbiorze, które mimo to ma mniejszy błąd na całym rozkładzie przypadków (tj. obejmującego także przykłady, które nie wystąpiły z zbiorze uczącym).</span>\n",
    "\n",
    "<span t=\"l1\">Drzewo przeuczone odzwierciedla przypadkowe przekłamania w danych lub zbyt szczegółowe regularności, nieistotne dla klasyfikacji przypadków.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013bdb4",
   "metadata": {},
   "source": [
    "### Upraszczanie (przycinanie drzewa)\n",
    "\n",
    "<span t=\"l1\">Usuwane są pewne fragmenty drzewa (tj. podrzewa) o niewielkim znaczeniu dla klasyfikacji.</span>\n",
    "\n",
    "<span t=\"l1\">W wyniku lokalnego usunięcia podrzewa z korzeniem tego podrzewa może być związany zbiór przykładów uczących należących do różnych klas decyzyjnych. Zmieniając taki węzeł na liść decyzyjny przypisuje mu się etykietę większościowej klasy decyzyjnej w zbiorze przykładów związanym z tym węzłem.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb0741",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Metody upraszczania:</span>\n",
    "- <span t=\"l2\">upraszczanie wstępne</span>\n",
    "- <span t=\"l2\">upraszczanie w pełni zbudowanego drzewa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18597b13",
   "metadata": {},
   "source": [
    "### Algorytm k-NN\n",
    "\n",
    "<span t=\"l1\">k-NN (k nearest neighbors) - algorytm k najbliższych sąsiadów jest algorytmem używanym do klasyfikacji nowych obiektów (przypadków).</span>\n",
    "\n",
    "<span t=\"l1\">Nowy obiekt porównywany jest z k najbliższymi sąsiadami ze zbioru uczącego.</span>\n",
    "\n",
    "<span t=\"l1\">Nowemu obiektowi przypisywana jest większościowa klasa sąsiadów ze zbioru uczącego.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2be320",
   "metadata": {},
   "source": [
    "<img src=\"../pliki_wlasne/knn.png\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7b9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
