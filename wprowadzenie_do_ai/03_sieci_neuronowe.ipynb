{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e4d023",
   "metadata": {},
   "source": [
    "# Sieci neuronowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad582bc",
   "metadata": {},
   "source": [
    "### Kategorie metod inspirowanych Naturą\n",
    "\n",
    "- <span t=\"l2\">Metody odtwarzające zjawiska występujące w Naturze (m.in. biologiczne), np. automaty komórkowe.</span>\n",
    "- <span t=\"l2\">Metody motywowane zjawiskami występującymi w Naturze (m.in. biologicznymi), np. algorytmy genetyczne, sieci neuronowe.</span>\n",
    "- <span t=\"l2\">Metody korzystające ze zjawisk występujących w Naturze (m.in. biologicznych) – obliczenia niekonwencjonalne.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bbf9c0",
   "metadata": {},
   "source": [
    "### Neuron biologiczny\n",
    "\n",
    "<img src=\"../pliki_z_internetu/neuron.png\" width=\"600\"/>\n",
    "\n",
    "- <span t=\"l2\">Neurony kontaktują się między sobą poprzez synapsy tworząc sieci neuronowe.</span>\n",
    "- <span t=\"l2\">Nośnikiem informacji są sygnały elektryczne.</span>\n",
    "- <span t=\"l2\">Dendryty → Synapsy położone na dendrytach służą do odbierania informacji z innych neuronów ↔ Wejścia.</span>\n",
    "- <span t=\"l2\">Zakończenia aksonu → Synapsy położone na na zakończeniach aksonu służą do przekazywania informacji do innych neuronów ↔ Wyjścia.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc80a7",
   "metadata": {},
   "source": [
    "### Od neuronu biologicznego do sztucznego neuronu\n",
    "\n",
    "<img src=\"../pliki_z_internetu/neuron2.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfdd0d",
   "metadata": {},
   "source": [
    "### Model neuronu\n",
    "<img src=\"../pliki_z_internetu/neuron3.jpg\" width=\"600\"/>\n",
    "\n",
    "- <span t=\"l2\">$y=f(\\sum_{i=0}^{n} w_i x_i)$</span>\n",
    "- <span t=\"l2\">f - funkcja aktywacji</span>\n",
    "- <span t=\"l2\">$x_1,x_2, ..., x_n$ - sygnały wejściowe</span>\n",
    "- <span t=\"l2\">$w_0, w_1, ..., w_n$ - wagi</span>\n",
    "- <span t=\"l2\">$y$ - sygnał wyjściowy</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e04a31",
   "metadata": {},
   "source": [
    "### Przykładowe funkcje aktywacji \n",
    "\n",
    "<span t=\"l1\">Unipolarna funkcja skokowa:</span>\n",
    "\n",
    "<span t=\"l2\">$f(z)=\\left\\{\n",
    "\\begin{array}{ccc}\n",
    "0&\\mbox{dla}&z<0\\\\\n",
    "1&\\mbox{dla}&z\\geq 0\n",
    "\\end{array}\n",
    "\\right.$</span>\n",
    "\n",
    "<span t=\"l2\">Bipolarna funkcja skokowa:</span>\n",
    "\n",
    "<span t=\"l2\">$f(z)=\\left\\{\n",
    "\\begin{array}{ccc}\n",
    "1&\\mbox{dla}&z>0\\\\\n",
    "-1&\\mbox{dla}&z\\leq 0\n",
    "\\end{array}\n",
    "\\right.$</span>\n",
    "\n",
    "\n",
    "<span t=\"l1\">Bipolarna funkcja sigmoidalna:</span>\n",
    "\n",
    "<span t=\"l2\">$f(z)=tanh(\\beta z)$</span>\n",
    "\n",
    "<span t=\"l1\">Obcięta funkcja liniowa</span>\n",
    "\n",
    "<span t=\"l2\">$f(z)=\\left\\{\n",
    "\\begin{array}{ccc}\n",
    "1&\\mbox{dla}&z>1\\\\\n",
    "z&\\mbox{dla}&-1\\leq z\\leq 1\\\\\n",
    "-1&\\mbox{dla}&z<-1\n",
    "\\end{array}\n",
    "\\right.$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a6ca4",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "- <span t=\"l1\">Perceptron to neuron z unipolarną skokową funkcją aktywacji.</span>\n",
    "- <span t=\"l1\">Perceptron najlepiej opisuje rzeczywisty neuron.</span>\n",
    "- <span t=\"l1\">Perceptron dokonuje jedynie liniowego podziału przestrzeni.</span>\n",
    "    - <span t=\"l2\">Przy jego pomocy nie można rozdzielić przypadków, które nie są liniowo separowalne.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe9a25",
   "metadata": {},
   "source": [
    "### Uczenie neuronu\n",
    "\n",
    "<span t=\"l1\">Uczenie nadzorowane (z nauczycielem)</span>\n",
    "\n",
    "<span t=\"l1\">Ciąg uczący (k przykładów):</span>\n",
    "- <span t=\"l2\">$(\\vec{x}(t), d(x(t))$</span>\n",
    "- <span t=\"l2\">$\\vec{x}(t)=[x_1(t),x_2(t), ..., x_n(t)]$ - wektor sygnałów wejściowych</span>\n",
    "- <span t=\"l2\">$d(\\vec{x}(t))$ – wartość wzorcowa sygnału wyjściowego dla wektora sygnałów wejściowych</span>\n",
    "- <span t=\"l2\">$t=1,2, ..., k$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5482d",
   "metadata": {},
   "source": [
    "<span t=\"l1\">1. Wybieramy w sposób losowy wagi początkowe neuronu. Ustawiamy $t=1$.</span>\n",
    "\n",
    "<span t=\"l1\">2. Na wejścia neuronu podajemy wektor uczący: $\\vec{x}(t)=[x_1(t),x_2(t), ..., x_n(t)]$</span>\n",
    "\n",
    "<span t=\"l1\">3. Obliczamy wartość wyjściową $y$ neuronu.</span>\n",
    "\n",
    "<span t=\"l1\">4. Porównujemy wartość wyjściową $y$ z wartością wzorcową $d(\\vec{x}(t))$ znajdującą się w ciągu uczącym.</span>\n",
    "\n",
    "<span t=\"l1\">5. Dokonujemy modyfikacji wag wg. zależności:</span>\n",
    "\n",
    "- <span t=\"l2\">jeżeli $y \\neq d(\\vec{x}(t)): w_{i}(t+1)=w_i(t)+(d(\\vec{x}(t))-y) \\cdot x_i(t)$</span>\n",
    "- <span t=\"l2\">jeżeli $y = d(\\vec{x}(t)): w_{i}(t+1)=w_i(t) $</span>\n",
    "\n",
    "<span t=\"l1\">6. Zwiększamy $t$ o 1 i jeśli $t$ nie jest większe od $k$ wracamy do punktu 2.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b636edb",
   "metadata": {},
   "source": [
    "- <span t=\"l1\">Algorytm uczenia powtarza się tak długo, aż dla wszystkich wektorów wejściowych wchodzących w skład ciągu uczącego błąd na wyjściu będzie mniejszy od założonej tolerancji.</span>\n",
    "- <span t=\"l1\">Wykonanie jednej pętli algorytmu dotyczy tzw. jednej epoki, na którą składają się dane tworzące ciąg uczący.</span>\n",
    "- <span t=\"l1\">Algorytm może być wielokrotnie stosowany dla tego samego ciągu uczącego, aż do spełnienia warunku zakończenia (konieczne może więc być podanie wielu epok).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735ec04",
   "metadata": {},
   "source": [
    "### Przykład 1\n",
    "\n",
    "Napisz skrypt dla środowiska R uczący perceptron funkcji sumy logicznej (OR). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ac91e",
   "metadata": {},
   "source": [
    "Krok 1: Definiujemy funkcję aktywacji (skok jednostkowy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f73bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit.step<-function(x)\n",
    "{\n",
    "  if(x>0.5) \n",
    "  {\n",
    "    return(1.0);\n",
    "  }\n",
    "  else \n",
    "  {\n",
    "    return(0.0);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb870b",
   "metadata": {},
   "source": [
    "Krok 2: Tworzymy tablicę decyzyjną reprezentującą funkcję sumy logicznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d01aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x1 x2 y\n",
      "1  0  0 0\n",
      "2  0  1 1\n",
      "3  1  0 1\n",
      "4  1  1 1\n"
     ]
    }
   ],
   "source": [
    "decision.table<-data.frame(matrix(c(0,0,0,0,1,1,1,0,1,1,1,1), ncol=3, byrow=TRUE));\n",
    "colnames(decision.table)<-c(\"x1\", \"x2\", \"y\");\n",
    "print(decision.table);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e8687",
   "metadata": {},
   "source": [
    "Krok 3: Wyodrębniamy z tablicy decyzyjnej atrubuty warunkowe (dwie pierwsze kolumny) i atrybut decyzyjny (ostania kolumna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86b3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=decision.table[,-c(3)]\n",
    "d=decision.table[,c(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd26efd",
   "metadata": {},
   "source": [
    "Krok 4: Losujemy wartości wektora wag (wartości z przedziału [0.0, 1.0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c8d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=runif(n=2, min=0.0, max=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4177841",
   "metadata": {},
   "source": [
    "Krok 5: Uczymy neuron iteracyjnie (pętla zewnętrzna dotyczy epok, pętla wewnętrzna przypadków w danej epoce)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482f2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for(e in 1:50)\n",
    "{\n",
    "  for(i in 1:nrow(x))\n",
    "  {\n",
    "    s=sum(w*x[i,]);\n",
    "    y=unit.step(s);\n",
    "\n",
    "    if(y!=d[i])\n",
    "    {\n",
    "      w=w+0.02*(d[i]-y)*x[i,];\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219623e",
   "metadata": {},
   "source": [
    "Krok 6: Wyświetlamy wektor wag po nauczeniu neuronu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d24ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         w1        w2\n",
      "3 0.4966881 0.1994433\n"
     ]
    }
   ],
   "source": [
    "colnames(w)<-c(\"w1\", \"w2\");\n",
    "print(w);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10a50c",
   "metadata": {},
   "source": [
    "Krok 7: Sprawdzamy jak nauczył się neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c10951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0\n",
      "[1] 1\n",
      "[1] 1\n",
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "for(i in 1:nrow(x))\n",
    "{\n",
    "  s=sum(w*x[i,]);\n",
    "  y=unit.step(s);\n",
    "  print(y);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a378e",
   "metadata": {},
   "source": [
    "### Przykład 2\n",
    "\n",
    "Napisz skrypt dla środowiska R uczący perceptron funkcji iloczynu logicznego (AND). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41349f21",
   "metadata": {},
   "source": [
    "Skrypt ma postać analogiczną jak w przykładzie 1 (różni się tablicą decyzyjną)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a20aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x1 x2 y\n",
      "1  0  0 0\n",
      "2  0  1 0\n",
      "3  1  0 0\n",
      "4  1  1 1\n",
      "         w1         w2\n",
      "3 0.4910686 0.02381247\n",
      "[1] 0\n",
      "[1] 0\n",
      "[1] 0\n",
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "unit.step<-function(x)\n",
    "{\n",
    "  if(x>0.5) \n",
    "  {\n",
    "    return(1.0);\n",
    "  }\n",
    "  else \n",
    "  {\n",
    "    return(0.0);\n",
    "  }\n",
    "}\n",
    "\n",
    "decision.table<-data.frame(matrix(c(0,0,0,0,1,0,1,0,0,1,1,1), ncol=3, byrow=TRUE));\n",
    "colnames(decision.table)<-c(\"x1\", \"x2\", \"y\");\n",
    "print(decision.table);\n",
    "\n",
    "x=decision.table[,-c(3)]\n",
    "d=decision.table[,c(3)]\n",
    "\n",
    "w=runif(n=2, min=0.0, max=1.0);\n",
    "\n",
    "for(e in 1:50)\n",
    "{\n",
    "  for(i in 1:nrow(x))\n",
    "  {\n",
    "    s=sum(w*x[i,]);\n",
    "    y=unit.step(s);\n",
    "\n",
    "    if(y!=d[i])\n",
    "    {\n",
    "      w=w+0.02*(d[i]-y)*x[i,];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "colnames(w)<-c(\"w1\", \"w2\");\n",
    "print(w);\n",
    "\n",
    "for(i in 1:nrow(x))\n",
    "{\n",
    "  s=sum(w*x[i,]);\n",
    "  y=unit.step(s);\n",
    "  print(y);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0a119",
   "metadata": {},
   "source": [
    "### Przykład 3\n",
    "\n",
    "Napisz skrypt dla środowiska R uczący perceptron funkcji alternatywy wykluczającej (EXOR). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fd35e",
   "metadata": {},
   "source": [
    "Skrypt ma postać analogiczną jak w przykładzie 1 (różni się tablicą decyzyjną)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a27f939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x1 x2 y\n",
      "1  0  0 0\n",
      "2  0  1 1\n",
      "3  1  0 1\n",
      "4  1  1 0\n",
      "        x1        x2\n",
      "3 0.330138 0.4885378\n",
      "[1] 0\n",
      "[1] 0\n",
      "[1] 0\n",
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "unit.step<-function(x)\n",
    "{\n",
    "  if(x>0.5) \n",
    "  {\n",
    "    return(1.0);\n",
    "  }\n",
    "  else \n",
    "  {\n",
    "    return(0.0);\n",
    "  }\n",
    "}\n",
    "\n",
    "decision.table<-data.frame(matrix(c(0,0,0,0,1,1,1,0,1,1,1,0), ncol=3, byrow=TRUE));\n",
    "colnames(decision.table)<-c(\"x1\", \"x2\", \"y\");\n",
    "print(decision.table);\n",
    "\n",
    "x=decision.table[,-c(3)]\n",
    "d=decision.table[,c(3)]\n",
    "\n",
    "w=runif(n=2, min=0.0, max=1.0);\n",
    "\n",
    "for(e in 1:50)\n",
    "{\n",
    "  for(i in 1:nrow(x))\n",
    "  {\n",
    "    s=sum(w*x[i,]);\n",
    "    y=unit.step(s);\n",
    "\n",
    "    if(y!=d[i])\n",
    "    {\n",
    "      w=w+0.02*(d[i]-y)*x[i,];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "print(w);\n",
    "\n",
    "for(i in 1:nrow(x))\n",
    "{\n",
    "  s=sum(w*x[i,]);\n",
    "  y=unit.step(s);\n",
    "  print(y);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d4419",
   "metadata": {},
   "source": [
    "UWAGA: Jeden neuron jest w stanie nauczyć się funkcji jeśli przypadki są liniowo separowalne. W przypadku funkcji EXOR tak nie jest i neuron nigdy jej się nie nauczy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93c78a",
   "metadata": {},
   "source": [
    "###  Wielowarstwowe sieci neuronowe\n",
    "\n",
    "<img src=\"../pliki_z_internetu/deep_net.png\" width=\"600\"/>\n",
    "\n",
    "<span t=\"l1\">WARSTWA WEJŚCIOWA - WARSTWA UKRYTA - WARSTWA WYJŚCIOWA</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba5646",
   "metadata": {},
   "source": [
    "<img src=\"../pliki_z_internetu/deep_net_2.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7f594",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Modyfikacja wag metodą wstecznej propagacji</span>\n",
    "\n",
    "\n",
    "<img src=\"../pliki_z_internetu/deep_net_3.png\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e3dbd",
   "metadata": {},
   "source": [
    "<span t=\"l1\">Błąd neuronu wyjściowego:</span>\n",
    "\n",
    "- <span t=\"l2\">$e_i=y_i(1-y_i)(d_i-y_i)$</span>\n",
    "\n",
    "\n",
    "<span t=\"l1\">Nowe wagi neuronu wyjściowego:</span>\n",
    "\n",
    "- <span t=\"l2\">$w'_{ji}=w_{ji}+e_i \\cdot h_j$</span>\n",
    "\n",
    "<span t=\"l1\">Błąd neuronu w warstwie ukrytej:</span>\n",
    "\n",
    "- <span t=\"l2\">$\\varepsilon_j=h_j(1-h_j)\\sum_{e_i}^{w_{ji}}$</span>\n",
    "\n",
    "<span t=\"l1\">Nowe wagi neuronu ukrytego:</span>\n",
    "- <span t=\"l2\">analogicznie jak dla neuronu wyjściowego</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0733c9c",
   "metadata": {},
   "source": [
    "### Inne modele sieci neuronowych\n",
    "\n",
    "- <span t=\"l1\">Sieci rekurencyjne, np. sieci Hopfielda, sieci Jordana, sieci Elmana.</span>\n",
    "- <span t=\"l1\">Sieci pamięci asocjacyjnej, np. sieci typu BAM, sieci Hamminga.</span>\n",
    "- <span t=\"l1\">Sieci samoorganizujące się, np. mapy samorganizujące się (SOM).</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244ed01",
   "metadata": {},
   "source": [
    "### Przykład sieci rekurencyjnej\n",
    "\n",
    "<img src=\"../pliki_z_internetu/siec_elmana.png\" width=\"600\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
