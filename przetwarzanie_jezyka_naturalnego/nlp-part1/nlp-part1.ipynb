{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego\n",
    "\n",
    "(Na podstawie książki Artificial Intelligence with Python autorstwa Prateek Joshi)\n",
    "\n",
    "W tym rozdziale nauczymy się przetwarzania języka naturalnego. Omówimy różne koncepcje, takie jak tokenizacja, wyprowadzanie i lematyzacja w celu przetwarzania tekstu. Następnie omówimy, jak zbudować model Bag of Words i wykorzystać go do klasyfikacji tekstu. Zobaczymy, jak wykorzystać uczenie maszynowe do analizy sentymentu danego zdania. Następnie omówimy modelowanie tematów i wdrożymy system identyfikacji tematów w danym dokumencie.\n",
    "\n",
    "Pod koniec tego rozdziału będziesz wiedział:\n",
    "- Jak zainstalować odpowiednie pakiety\n",
    "- Tokenizacja danych tekstowych\n",
    "- Konwertowanie słów na ich formy podstawowe przy użyciu tematyki konwersyjnej \n",
    "- Konwertowanie słów na ich formy podstawowe za pomocą lematyzacji Dzielenie danych tekstowych na fragmenty\n",
    "- Wyodrębnianie macierzy terminów dokumentu za pomocą modelu Bag of Words \n",
    "- Budowanie predyktora kategorii\n",
    "- Konstruowanie identyfikatora płci\n",
    "- Budowanie analizatora nastrojów (sentymentu)\n",
    "- Modelowanie tematyczne z wykorzystaniem ukrytej alokacji Dirichleta\n",
    "\n",
    "## Wprowadzenie i instalacja pakietów\n",
    "Przetwarzanie języka naturalnego (NLP) stało się ważną częścią nowoczesnych systemów. Jest szeroko stosowany w wyszukiwarkach, interfejsach konwersacyjnych, procesorach dokumentów i tak dalej. Maszyny dobrze radzą sobie z danymi strukturalnymi. Ale jeśli chodzi o pracę z tekstem o dowolnej formie, mają trudności. Celem NLP jest opracowanie algorytmów, które umożliwią komputerom zrozumienie dowolnego tekstu i pomogą im zrozumieć język.\n",
    "\n",
    "Jedną z najtrudniejszych rzeczy w przetwarzaniu dowolnego języka naturalnego jest ogromna liczba jego odmian. Kontekst odgrywa bardzo ważną rolę w zrozumieniu danego zdania. Ludzie są w tym wspaniali, ponieważ byliśmy szkoleni przez wiele lat. Natychmiast wykorzystujemy naszą przeszłą wiedzę, aby zrozumieć kontekst i wiedzieć, o czym mówi druga osoba.\n",
    "\n",
    "Aby rozwiązać ten problem, badacze NLP zaczęli opracowywać różne aplikacje z wykorzystaniem metod uczenia maszynowego. Aby zbudować takie aplikacje, musimy zebrać duży korpus tekstu, a następnie wyszkolić algorytm do wykonywania różnych zadań, takich jak kategoryzowanie tekstu, analizowanie sentymentów lub modelowanie tematów. Te algorytmy są uczone do wykrywania wzorców w wejściowych danych tekstowych i uzyskiwania z nich szczegółowych informacji.\n",
    "W tym rozdziale omówimy różne podstawowe pojęcia, które są używane do analizy tekstu i tworzenia aplikacji NLP. To pozwoli nam zrozumieć, jak wydobyć znaczące informacje z podanych danych tekstowych. Do tworzenia tych aplikacji użyjemy pakietu Pythona o nazwie Natural Language Toolkit (NLTK). Upewnij się, że zainstalowałeś."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/robert.trypuz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim==3.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja danych tekstowych\n",
    "\n",
    "Kiedy mamy do czynienia z tekstem, musimy podzielić go na mniejsze części do analizy. W tym miejscu pojawia się tokenizacja. Jest to proces dzielenia tekstu wejściowego na zestaw części, takich jak słowa lub zdania. Te elementy nazywane są żetonami. W zależności od tego, co chcemy zrobić, możemy zdefiniować własne metody podziału tekstu na wiele tokenów. Przyjrzyjmy się, jak tokenizować tekst wejściowy za pomocą NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" \n",
    "\n",
    "# Sentence tokenizer \n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# Word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "# WordPunct tokenizer\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja słów do ich form podstawowych\n",
    "\n",
    "Praca z tekstem ma wiele odmian. Musimy radzić sobie z różnymi formami tego samego słowa i umożliwić komputerowi zrozumienie, że te różne słowa mają tę samą formę podstawową. Na przykład słowo śpiewać może występować w wielu formach, takich jak śpiew, piosenkarz, śpiew, piosenkarz i tak dalej. Właśnie widzieliśmy zestaw słów o podobnym znaczeniu. Ludzie mogą łatwo zidentyfikować te podstawowe formy i wyprowadzić kontekst.\n",
    "\n",
    "Kiedy analizujemy tekst, warto wyodrębnić te podstawowe formy. Umożliwi nam to uzyskanie przydatnych statystyk do analizy wprowadzonego tekstu. Stemming jest jednym ze sposobów osiągnięcia tego. Celem stemmera jest zredukowanie słów w ich różnych formach do wspólnej formy podstawowej. Jest to w zasadzie proces heurystyczny, który odcina końce słów, aby wyodrębnić ich podstawowe formy. Zobaczmy, jak to zrobić za pomocą NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "          horses            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'horses', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), \n",
    "        '\\n', '='*68)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porozmawiajmy trochę o trzech algorytmach rdzenia, które są tutaj używane. W zasadzie wszyscy starają się osiągnąć ten sam cel. Różnica między nimi polega na poziomie ścisłości, który jest używany do uzyskania formy podstawowej.\n",
    "\n",
    "Stemmer Porter  jest najmniej rygorystyczna, a Lancaster najsurowsza. Jeśli uważnie przyjrzysz się wynikom, zauważysz różnice. Stemmery zachowują się inaczej, jeśli chodzi o słowa takie jak possibly lub provision. Wyjścia stemplowane, które są uzyskiwane z lancastera Lancaster są nieco zaciemnione, ponieważ bardzo redukują słowa. Jednocześnie algorytm jest naprawdę szybki. Dobrą zasadą jest użycie łodygi Snowball, ponieważ jest to dobry kompromis między szybkością a ścisłością."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja słów do ich form podstawowych za pomocą lematyzacji\n",
    "\n",
    "Lematyzacja to kolejny sposób na zredukowanie słów do ich podstawowych form. W poprzedniej sekcji widzieliśmy, że formy podstawowe, które uzyskano z tych macierzystych, nie miały sensu. Na przykład wszyscy trzej hodowcy powiedzieli, że podstawową formą calves jest calv, co nie jest prawdziwym słowem. Lematyzacja wymaga bardziej uporządkowanego podejścia do rozwiązania tego problemu.\n",
    "W procesie lematyzacji wykorzystuje się słownictwo i analizę morfologiczną słów. Formy podstawowe uzyskuje poprzez usunięcie końcówek fleksyjnych, takich jak ing czy ed. Ta podstawowa forma dowolnego słowa jest znana jako lemat. Jeśli lemmatyzujesz słowo calves, powinieneś otrzymać calf jako wynik. Należy zauważyć, że wynik zależy od tego, czy słowo jest czasownikiem, czy rzeczownikiem. Przyjrzyjmy się, jak to zrobić za pomocą NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                     are                     are                      be\n",
      "                      is                      is                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "                  horses                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'are', 'is', 'branded', 'horse', 'horses','randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create lemmatizer object \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
    "        '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
    "           lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dzielenie danych tekstowych na porcje\n",
    "\n",
    "W celu dalszej analizy dane tekstowe zwykle trzeba podzielić na części. Ten proces jest nazywany fragmentacją. Jest to często używane w analizie tekstu. Warunki używane do podzielenia tekstu na fragmenty mogą się różnić w zależności od problemu. To nie to samo, co tokenizacja, w której również dzielimy tekst na części. Podczas dzielenia na fragmenty nie stosujemy się do żadnych ograniczeń, a fragmenty wyjściowe muszą mieć znaczenie.\n",
    "Kiedy mamy do czynienia z dużymi dokumentami tekstowymi, ważne staje się podzielenie tekstu na fragmenty, aby wydobyć znaczące informacje. W tej sekcji zobaczymy, jak podzielić tekst wejściowy na kilka części."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input text into chunks, where each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produ\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city officials to pass enabling legislation that will per\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned the State Highway Department is very near being read\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threats of violence . The former county school superinte\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451,500 , which would be a savings of $157,460 yearly\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposal would have to receive final legislative approval\n",
      "Chunk 7 ==> College . He has served as a border patrolman and was in the Signal Corps of the U.S. Army . Denton \n",
      "Chunk 8 ==> of his staff were doing on the address involved composition and wording , rather than last minute de\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year and the payroll tax to 6.5 per cent -- 3.25 per cen\n",
      "Chunk 10 ==> nursing homes In the area of `` community health services '' , the President called for doubling the\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been a noticeable relaxation of tension . The general ,\n",
      "Chunk 12 ==> system which will prevent Laos from being used as a base for Communist attacks on neighboring Thaila\n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the administration looked at the Eisenhower administration e\n",
      "Chunk 14 ==> . He is not interested in being named a full-time director . Noting that President Kennedy has hande\n",
      "Chunk 15 ==> said , `` to obtain the views of the general public and religious , labor and special-interest group\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , is engaged in industrial relations counseling . A p\n",
      "Chunk 17 ==> making enforcement of minor offenses more effective . Nothing has been done yet to take advantage of\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issue '' . Defends Ike Earlier , Mitchell said in a st\n"
     ]
    }
   ],
   "source": [
    "# Read the first 12000 words from the Brown corpus\n",
    "input_data = ' '.join(brown.words()[:12000])\n",
    "\n",
    "# Define the number of words in each chunk \n",
    "chunk_size = 700\n",
    "\n",
    "chunks = chunker(input_data, chunk_size)\n",
    "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk', i+1, '==>', chunk[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. . `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' . The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' . It recommended that Fulton legislators act `` to have these laws studied and revised to the end of modernizing and improving them '' . The grand jury commented on a number of other topics , among them the Atlanta and Fulton County purchasing departments which it said `` are well operated and follow generally accepted practices which inure to the best interest of both governments '' . Merger proposed However , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' . The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' . It urged that the city `` take steps to remedy '' this problem . Implementation of Georgia's automobile title law was also recommended by the outgoing jury . It urged that the next Legislature `` provide enabling funds and re-set the effective date so that an orderly implementation of the law may be effected '' . The grand jury took a swipe at the State Welfare Department's handling of federal funds granted for child welfare services in foster homes . `` This is one of the major items in the Fulton County general assistance program '' , the jury said , but the State Welfare Department `` has seen fit to distribute these funds through the welfare departments of all the counties in the state with the exception of Fulton County , which receives none of this money . The jurors said they realize `` a proportionate distribution of these funds might disable this program in our less populous counties '' . Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said . `` Failure to do this will continue to place a disproportionate burden '' on Fulton taxpayers . The jury also commented on the Fulton ordinary's court which has been under fire for its practices in the appointment of appraisers , guardians and administrators and the awarding of fees and compensation . Wards protected The jury said it found the court `` has incorporated into its operating procedures the recommendations '' of two previous grand juries , the Atlanta Bar Association and an interim citizens committee . `` These actions should serve to protect in fact and in effect the court's wards from undue costs and its appointed and elected servants from unmeritorious criticisms '' , the jury said . Regarding Atlanta's new multi-million-dollar airport , the jury recommended `` that when the new management takes charge Jan. 1 the airport be operated in a manner that will eliminate political influences '' . The jury did not elaborate , but it added that `` there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable '' . Ask jail deputies On other matters , the jury recommended that : ( 1 ) Four additional deputies be employed at the Fulton County Jail and `` a doctor , medical intern or extern be employed for night and weekend duty at the jail\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyodrębnianie częstotliwości terminów za pomocą modelu Bag of Words\n",
    "\n",
    "Jednym z głównych celów analizy tekstu jest konwersja tekstu do postaci numerycznej, abyśmy mogli wykorzystać na nim uczenie maszynowe. Rozważmy dokumenty tekstowe, które zawierają wiele milionów słów. Aby przeanalizować te dokumenty, musimy wyodrębnić tekst i przekształcić go w formę reprezentacji numerycznej.\n",
    "\n",
    "Algorytmy uczenia maszynowego potrzebują danych liczbowych do pracy, aby móc analizować dane i wyodrębniać znaczące informacje. W tym miejscu pojawia się model Bag of Words. Model ten wyodrębnia słownictwo ze wszystkich słów w dokumentach i buduje model przy użyciu matrycy terminów dokumentu. To pozwala nam przedstawić każdy dokument jako zbiór słów. Po prostu śledzimy liczbę słów i pomijamy szczegóły gramatyczne i kolejność słów.\n",
    "\n",
    "Zobaczmy, o co chodzi w macierzy terminów dokumentu. Macierz terminów dokumentu to w zasadzie tabela, która podaje liczbę różnych słów występujących w dokumencie. Tak więc dokument tekstowy można przedstawić jako ważoną kombinację różnych słów. Możemy ustawić progi i wybrać słowa, które są bardziej znaczące. W pewnym sensie tworzymy histogram wszystkich słów w dokumencie, który będzie używany jako wektor cech. Ten wektor cech jest używany do klasyfikacji tekstu.\n",
    "\n",
    "Rozważ następujące zdania:\n",
    "- Sentence 1: The children are playing in the hall\n",
    "- Sentence 2: The hall has a lot of space\n",
    "- Sentence 3: Lots of children like playing in an open space\n",
    "\n",
    "Jeśli weźmiesz pod uwagę wszystkie trzy zdania, mamy dziewięć niepowtarzalnych słów:\n",
    "- the \n",
    "- children \n",
    "- are \n",
    "- playing \n",
    "- in\n",
    "- hall\n",
    "- has\n",
    "- a\n",
    "- lot\n",
    "- of \n",
    "- space \n",
    "- like \n",
    "- an \n",
    "- open\n",
    "\n",
    "Jest tutaj 14 różnych słów. Skonstruujmy histogram dla każdego zdania, używając liczby słów w każdym zdaniu. Każdy wektor cech będzie 14-wymiarowy, ponieważ w sumie mamy 14 różnych słów:\n",
    "\n",
    "- Sentence 1: [2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] \n",
    "- Sentence 2: [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0] \n",
    "- Sentence 3: [0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "Teraz, gdy wyodrębniliśmy te wektory cech, możemy użyć algorytmów uczenia maszynowego do analizy tych danych.\n",
    "Zobaczmy, jak zbudować model Bag of Words w NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the Brown corpus\n",
    "input_data = ' '.join(brown.words()[:5400])\n",
    "\n",
    "# Number of words in each chunk \n",
    "chunk_size = 800\n",
    "\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "\n",
    "# Convert to dict items\n",
    "chunks = []\n",
    "for count, chunk in enumerate(text_chunks):\n",
    "    d = {'index': count, \n",
    "         'text': chunk}\n",
    "    chunks.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the document term matrix\n",
    "# min_df=7 - we ignore words that appear in less than 7 docs\n",
    "# max_df=20 - we ignore words that appear in more than 20 docs\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x21 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 147 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      " ['and' 'are' 'be' 'by' 'county' 'for' 'in' 'is' 'it' 'of' 'on' 'one'\n",
      " 'said' 'state' 'that' 'the' 'to' 'two' 'was' 'which' 'with']\n"
     ]
    }
   ],
   "source": [
    "# Extract the vocabulary and display it\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'be', 'by', 'county', 'for', 'in', 'is', 'it', 'of',\n",
       "       'on', 'one', 'said', 'state', 'that', 'the', 'to', 'two', 'was',\n",
       "       'which', 'with'], dtype='<U6')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names for chunks\n",
    "chunk_names = []\n",
    "for i in range(len(text_chunks)):\n",
    "    chunk_names.append('Chunk-' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71,  6, 12, 31, 13, 15,  7, 23,  7,  5,  3, 11,  8,  2,  4,  2,  6,\n",
       "        2,  3,  1,  2, 51,  2,  5, 20,  8, 11,  4,  9, 13,  6,  4, 26,  6,\n",
       "        2,  3,  1,  8,  7,  7,  3,  2, 43,  7,  7, 20,  9, 15,  5,  9,  4,\n",
       "        7,  4, 20,  8,  1,  5,  1,  7,  3,  2,  1,  3, 51,  3,  7, 30,  2,\n",
       "       11,  4, 11, 10,  7,  5, 26,  9,  1, 10,  1,  7,  4,  6,  2,  1, 43,\n",
       "        1,  4, 29,  7, 13,  3,  9,  7,  4, 14, 21,  3,  2,  6,  1,  6,  5,\n",
       "        3,  2,  2, 52,  2,  3, 35,  1, 14,  1, 17,  6,  7,  3, 15,  1,  2,\n",
       "        5,  2,  2,  5,  4,  1,  2, 49,  2,  7, 26,  7, 17,  1, 10,  4,  3,\n",
       "        6, 11,  2,  1,  2,  2,  1,  2,  1,  1,  3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-1     Chunk-2     Chunk-3     Chunk-4     Chunk-5     Chunk-6     Chunk-7 \n",
      "\n",
      "         and          23           9           9          11           9          17          10\n",
      "         are           2           2           1           1           2           2           1\n",
      "          be           6           8           7           7           6           2           1\n",
      "          by           3           4           4           5          14           3           6\n",
      "      county           6           2           7           3           1           2           2\n",
      "         for           7          13           4          10           7           6           4\n",
      "          in          15          11          15          11          13          14          17\n",
      "          is           2           7           3           4           5           5           2\n",
      "          it           8           6           8           9           3           1           2\n",
      "          of          31          20          20          30          29          35          26\n",
      "          on           4           3           5          10           6           5           2\n",
      "         one           1           3           1           2           2           1           1\n",
      "        said          12           5           7           7           4           3           7\n",
      "       state           3           7           2           6           3           4           1\n",
      "        that          13           8           9           2           7           1           7\n",
      "         the          71          51          43          51          43          52          49\n",
      "          to          11          26          20          26          21          15          11\n",
      "         two           2           1           1           1           1           2           2\n",
      "         was           5           6           7           7           4           7           3\n",
      "       which           7           4           5           4           3           1           1\n",
      "        with           2           2           3           1           2           2           3\n"
     ]
    }
   ],
   "source": [
    "# Print the document term matrix\n",
    "print(\"\\nDocument term matrix:\")\n",
    "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_text.format('Word', *chunk_names), '\\n')\n",
    "for word, item in zip(vocabulary, document_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowanie predyktora kategorii\n",
    "\n",
    "**Predyktor kategorii służy do przewidywania kategorii, do której należy dany fragment tekstu**. Jest to często używane w klasyfikacji tekstu do kategoryzowania dokumentów tekstowych. Wyszukiwarki często używają tego narzędzia do porządkowania wyników wyszukiwania według trafności. Załóżmy na przykład, że chcemy przewidzieć, czy dane zdanie należy do sportu, polityki czy nauki. Aby to zrobić, budujemy korpus danych i trenujemy algorytm. Algorytm ten można następnie wykorzystać do wnioskowania o nieznanych danych.\n",
    "\n",
    "Aby zbudować ten predyktor, użyjemy miary zwanej **TermFrequency - Inverse Document Frequency (tf-idf)**. W zestawie dokumentów musimy zrozumieć znaczenie każdego słowa. Statystyka tf-idf pomaga nam zrozumieć, jak ważne jest dane słowo dla dokumentu w zestawie dokumentów.\n",
    "\n",
    "Rozważmy pierwszą część tej miary. **tf dla danego terminu $t$ i dokumentu $d$ wyliczmay dzieląc liczbą wystąpień termu $t$ w dokumencie $d$ przez sumę liczby wystąpień wszystkich termów w dokumencie $d$**. Częstotliwość terminu (tf) jest w zasadzie miarą tego, jak często każde słowo pojawia się w danym dokumencie. Ponieważ różne dokumenty mają różną liczbę słów, dokładne liczby na histogramie będą się różnić. Aby mieć równe szanse, musimy znormalizować histogramy. Więc dzielimy liczbę każdego słowa przez całkowitą liczbę słów w danym dokumencie, aby otrzymać **częstotliwość terminu**.\n",
    "\n",
    "Druga część miary to odwrotna częstotliwość dokumentów (idf). **idf, dla danego terminu $t$ i zbioru dokumentów $D$, obliczmay dzieląc liczbę dokumentów w korpusie przez liczba dokumentów w których ten termin (przynajmniej raz) występuje - wynik logarytmizujemy.** idf jest miarą unikalności słowa w dokumencie w kontekście zestawiu/korpusu dokumentów. Kiedy oblicza się częstotliwość terminu, zakłada się, że wszystkie słowa są jednakowo ważne. Ale nie możemy po prostu polegać na częstotliwości każdego słowa, ponieważ słowa takie jak \"i\" pojawiają się często. Aby zrównoważyć częstotliwości tych powszechnie występujących słów, musimy zmniejszyć ich wagę i zważyć rzadkie słowa. Pomaga nam to również zidentyfikować słowa, które są unikalne dla każdego dokumentu, co z kolei pomaga nam sformułować charakterystyczny wektor cech. idf jest zasadniczo ułamkiem dokumentów, które zawierają dane słowo. \n",
    "\n",
    "Następnie łączymy częstotliwość terminów i odwrotną częstotliwość dokumentów, aby sformułować wektor cech do kategoryzacji dokumentów. Zobaczmy, jak zbudować predyktor kategorii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. A sentence is typically associated with a clause and a clause can be either a clause simplex or a clause complex.\n",
      "1. A clause is a clause simplex if it represents a single process going on through time and it is a clause complex if it represents a logical relation between two or more processes and is thus composed of two or more clause simplexes.\n",
      "2. A clause (simplex) typically contains a predication structure with a subject noun phrase and a finite verb.\n",
      "3. Although the subject is usually a noun phrase, other kinds of phrases (such as gerund phrases) work as well, and some languages allow subjects to be omitted.\n",
      "4. In the examples below, the subject of the outmost clause simplex is in italics and the subject of boiling is in square brackets.\n",
      "5. Notice that there is clause embedding in the second and third examples.\n",
      "6. [Water] boils at 100 degrees Celsius.\n",
      "7. It is quite interesting that [water] boils at 100 degrees Celsius.\n",
      "8. The fact that [water] boils at 100 degrees Celsius is quite interesting.\n",
      "9. There are two types of clauses: independent and non-independent/interdependent.\n",
      "10. An independent clause realises a speech act such as a statement, a question, a command or an offer.\n",
      "11. A non-independent clause does not realise any act.\n",
      "12. A non-independent clause (simplex or complex) is usually logically related to other non-independent clauses.\n",
      "13. Together they usually constitute a single independent clause (complex).\n",
      "14. For that reason, non-independent clauses are also called interdependent.\n",
      "15. For instance, the non-independent clause because I have no friends is related to the non-independent clause I don't go out in I don't go out, because I have no friends.\n",
      "16. The whole clause complex is independent because it realises a statement.\n",
      "17. What is stated is the causal nexus between having no friend and not going out.\n",
      "18. When such a statement is acted out, the fact that the speaker doesn't go out is already established, therefore it cannot be stated.\n",
      "19. What is still open and under negotiation is the reason for that fact.\n",
      "20. The causal nexus is represented by the independent clause complex and not by the two interdependent clause simplexes.\n",
      "tf = 0.25\n",
      "idf = 0.56\n",
      "tfidf = 0.14\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "\n",
    "text = '''A sentence is typically associated with a clause and a clause can be either a clause simplex or a clause complex. A clause is a clause simplex if it represents a single process going on through time and it is a clause complex if it represents a logical relation between two or more processes and is thus composed of two or more clause simplexes.\n",
    "\n",
    "A clause (simplex) typically contains a predication structure with a subject noun phrase and a finite verb. Although the subject is usually a noun phrase, other kinds of phrases (such as gerund phrases) work as well, and some languages allow subjects to be omitted. In the examples below, the subject of the outmost clause simplex is in italics and the subject of boiling is in square brackets. Notice that there is clause embedding in the second and third examples.\n",
    "\n",
    "[Water] boils at 100 degrees Celsius.\n",
    "It is quite interesting that [water] boils at 100 degrees Celsius.\n",
    "The fact that [water] boils at 100 degrees Celsius is quite interesting.\n",
    "There are two types of clauses: independent and non-independent/interdependent. An independent clause realises a speech act such as a statement, a question, a command or an offer. A non-independent clause does not realise any act. A non-independent clause (simplex or complex) is usually logically related to other non-independent clauses. Together they usually constitute a single independent clause (complex). For that reason, non-independent clauses are also called interdependent. For instance, the non-independent clause because I have no friends is related to the non-independent clause I don't go out in I don't go out, because I have no friends. The whole clause complex is independent because it realises a statement. What is stated is the causal nexus between having no friend and not going out. When such a statement is acted out, the fact that the speaker doesn't go out is already established, therefore it cannot be stated. What is still open and under negotiation is the reason for that fact. The causal nexus is represented by the independent clause complex and not by the two interdependent clause simplexes.\n",
    "'''\n",
    "docs = sent_tokenize(text) # 21 zdań\n",
    "\n",
    "for index, doc in enumerate(docs):\n",
    "    print(str(index)+\".\", doc)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "doc_term_matrix = count_vectorizer.fit_transform([doc for doc in docs])\n",
    "vocabulary = count_vectorizer.get_feature_names()\n",
    "\n",
    "def tf(term, doc_nr, vocabulary, doc_term_matrix):\n",
    "    term = term.lower()\n",
    "    tf=0\n",
    "    if term in vocabulary:\n",
    "        term_index = vocabulary.index(term)\n",
    "        t_number = doc_term_matrix[doc_nr, term_index]\n",
    "        all_occ = 0\n",
    "        for position in range(len(vocabulary)):\n",
    "            all_occ += doc_term_matrix[doc_nr, position]\n",
    "    return t_number/all_occ\n",
    "\n",
    "def idf(term, docs, vocabulary, doc_term_matrix):\n",
    "    term_index = vocabulary.index(term)\n",
    "    number_of_docs = len(docs)\n",
    "    docs_with_term = 0\n",
    "    for doc_nr in range(number_of_docs):\n",
    "        if doc_term_matrix[doc_nr, term_index] > 0:\n",
    "            docs_with_term += 1  \n",
    "    return math.log(number_of_docs / docs_with_term)\n",
    "\n",
    "def tfidf(term, doc_nr, docs, vocabulary, doc_term_matrix):\n",
    "    tf_result = tf(term, doc_nr, vocabulary, doc_term_matrix)\n",
    "    idf_result = idf(term, docs, vocabulary, doc_term_matrix)\n",
    "    return tf_result*idf_result\n",
    "\n",
    "doc_nr = 0\n",
    "word = \"clause\"\n",
    "\n",
    "tf_result = tf(word, doc_nr, vocabulary, doc_term_matrix)\n",
    "print(\"tf =\",round(tf_result,2))\n",
    "\n",
    "idf_result = idf(word, docs, vocabulary, doc_term_matrix)\n",
    "print(\"idf =\",round(idf_result,2))\n",
    "\n",
    "tfidf_result = tfidf(word, doc_nr, docs, vocabulary, doc_term_matrix)\n",
    "print(\"tfidf =\",round(tfidf_result,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19 ( 0.06 3.04 ) sentence\n",
      "0.03 ( 0.06 0.41 ) is\n",
      "0.15 ( 0.06 2.35 ) typically\n",
      "0.19 ( 0.06 3.04 ) associated\n",
      "0.15 ( 0.06 2.35 ) with\n",
      "0.14 ( 0.25 0.56 ) clause\n",
      "0.05 ( 0.06 0.74 ) and\n",
      "0.14 ( 0.25 0.56 ) clause\n",
      "0.19 ( 0.06 3.04 ) can\n",
      "0.12 ( 0.06 1.95 ) be\n",
      "0.19 ( 0.06 3.04 ) either\n",
      "0.14 ( 0.25 0.56 ) clause\n",
      "0.09 ( 0.06 1.44 ) simplex\n",
      "0.1 ( 0.06 1.66 ) or\n",
      "0.14 ( 0.25 0.56 ) clause\n",
      "0.08 ( 0.06 1.25 ) complex\n"
     ]
    }
   ],
   "source": [
    "doc_nr = 0\n",
    "for word in word_tokenize(docs[doc_nr]):\n",
    "    if len(word) > 1:\n",
    "        tf_result = tf(word, doc_nr, vocabulary, doc_term_matrix)\n",
    "        idf_result = idf(word, docs, vocabulary, doc_term_matrix)\n",
    "        tfidf_result = tfidf(word, doc_nr, docs, vocabulary, doc_term_matrix)\n",
    "        print(round(tfidf_result,2), \"(\", round(tf_result,2), round(idf_result,2),\")\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_20newsgroups in module sklearn.datasets._twenty_newsgroups:\n",
      "\n",
      "fetch_20newsgroups(*, data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False)\n",
      "    Load the filenames and data from the 20 newsgroups dataset (classification).\n",
      "    \n",
      "    Download it if necessary.\n",
      "    \n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "    \n",
      "    Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : optional, default: None\n",
      "        Specify a download and cache folder for the datasets. If None,\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    subset : 'train' or 'test', 'all', optional\n",
      "        Select the dataset to load: 'train' for the training set, 'test'\n",
      "        for the test set, 'all' for both, with shuffled ordering.\n",
      "    \n",
      "    categories : None or collection of string or unicode\n",
      "        If None (default), load all the categories.\n",
      "        If not None, list of category names to load (other categories\n",
      "        ignored).\n",
      "    \n",
      "    shuffle : bool, optional\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    random_state : int, RandomState instance, default=None\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    remove : tuple\n",
      "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "        these are kinds of text that will be detected and removed from the\n",
      "        newsgroup posts, preventing classifiers from overfitting on\n",
      "        metadata.\n",
      "    \n",
      "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "        ends of posts that look like signatures, and 'quotes' removes lines\n",
      "        that appear to be quoting another post.\n",
      "    \n",
      "        'headers' follows an exact standard; the other filters are not always\n",
      "        correct.\n",
      "    \n",
      "    download_if_missing : optional, True by default\n",
      "        If False, raise an IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    return_X_y : bool, default=False.\n",
      "        If True, returns `(data.data, data.target)` instead of a Bunch\n",
      "        object.\n",
      "    \n",
      "        .. versionadded:: 0.22\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    bunch : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : list, length [n_samples]\n",
      "            The data list to learn.\n",
      "        target: array, shape [n_samples]\n",
      "            The target labels.\n",
      "        filenames: list, length [n_samples]\n",
      "            The path to the location of the data.\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "        target_names: list, length [n_classes]\n",
      "            The names of target classes.\n",
      "    \n",
      "    (data, target) : tuple if `return_X_y=True`\n",
      "        .. versionadded:: 0.22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the category map\n",
    "category_map = {'talk.politics.misc': 'Politics', \n",
    "                'rec.autos': 'Autos', \n",
    "                'rec.sport.hockey': 'Hockey', \n",
    "                'sci.electronics': 'Electronics', \n",
    "                'sci.med': 'Medicine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training dataset\n",
    "training_data = fetch_20newsgroups(subset='train', \n",
    "                                   categories=category_map.keys(), \n",
    "                                   shuffle=True, \n",
    "                                   random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: cramer@optilink.COM (Clayton Cramer)\\nSubject: Re: Why not concentrate on child molesters?\\nArticle-I.D.: optilink.15213\\nOrganization: Optilink Corporation, Petaluma, CA\\nLines: 20\\n\\nIn article <7166@pdxgate.UUCP>, a0cb@rigel.cs.pdx.edu (Chris Bertholf) writes:\\n> MCARTWR@auvm.american.edu (Martina Cartwright) writes:\\n# #The official and legal term for rape is \"the crime of forcing a FEMALE \\n# #to submit to sexual intercourse.\"\\n# \\n# Please, supply me with some references.  I was not aware that all states\\n# had the word \"FEMALE\" in the rape statutes.  I am sure others are surprised\\n# as well.  I know thats how it works in practice (nice-n-fair, NOT!!), but\\n# was unaware that it was in the statutes as applying to FEMALES only,\\n# uniformly throughout the U.S.\\n# \\n# -Chris\\n\\nThere may be some confusion here.  The Uniform Crime Reports program\\nrun by the FBI defines rape as a female victim only crime -- even\\nthough some states have the laws de-sexed.  I suspect that this causes\\nmale victims of rape to be left out of the UCR data.\\n-- \\nClayton E. Cramer {uunet,pyramid}!optilink!cramer  My opinions, all mine!\\nRelations between people to be by mutual consent, or not at all.\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec.autos',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'talk.politics.misc']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2844, 40321)\n"
     ]
    }
   ],
   "source": [
    "# Build a count vectorizer and extract term counts \n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data:\", train_tc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: You need to be careful with cars when you are driving on slippery roads \n",
      "Predicted category: Autos\n",
      "\n",
      "Input: A lot of devices can be operated wirelessly \n",
      "Predicted category: Electronics\n",
      "\n",
      "Input: Players need to be careful when they are close to goal posts \n",
      "Predicted category: Hockey\n",
      "\n",
      "Input: Political debates help us understand the perspectives of both sides \n",
      "Predicted category: Politics\n",
      "\n",
      "Input: Citroën intends to reinforce Berlingo's adventurous spirit with this special edition, through an expressive and distinctive version, for a way of life geared towards freedom and action. Based on the core Feel Pack version of the range, the Berlingo Rip Curl has adopted an even more dynamic appearance with specific colour schemes both inside and out, and has been enhanced for the occasion by a number of features for even greater on-board comfort. \n",
      "Predicted category: Autos\n"
     ]
    }
   ],
   "source": [
    "# Define test data \n",
    "input_data = [\n",
    "    'You need to be careful with cars when you are driving on slippery roads', \n",
    "    'A lot of devices can be operated wirelessly',\n",
    "    'Players need to be careful when they are close to goal posts',\n",
    "    'Political debates help us understand the perspectives of both sides',\n",
    "    'Citroën intends to reinforce Berlingo\\'s adventurous spirit with this special edition, through an expressive and distinctive version, for a way of life geared towards freedom and action. Based on the core Feel Pack version of the range, the Berlingo Rip Curl has adopted an even more dynamic appearance with specific colour schemes both inside and out, and has been enhanced for the occasion by a number of features for even greater on-board comfort.'\n",
    "]\n",
    "\n",
    "# Transform input data using count vectorizer\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "\n",
    "# Transform vectorized data using tfidf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "# Predict the output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nInput:', sent, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstruowanie identyfikatora płci w oparciu o imiona\n",
    "\n",
    "Identyfikacja płci jest interesującym problemem. W tym przypadku użyjemy heurystyki do skonstruowania wektora cech i użyjemy go do wytrenowania klasyfikatora. Heurystyka, która zostanie tutaj użyta, to ostatnie N liter danej nazwy. Na przykład, jeśli imię kończy się na \"ia\", najprawdopodobniej jest to imię żeńskie, na przykład Amelia lub Genelia. Z drugiej strony, jeśli imię kończy się na \"rk\", prawdopodobnie jest to imię męskie, takie jak Mark lub Clark. Ponieważ nie jesteśmy pewni dokładnej liczby liter, których należy użyć, będziemy bawić się tym parametrem, aby dowiedzieć się, jaka jest najlepsza odpowiedź."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/robert.trypuz/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aamir',\n",
       " 'Aaron',\n",
       " 'Abbey',\n",
       " 'Abbie',\n",
       " 'Abbot',\n",
       " 'Abbott',\n",
       " 'Abby',\n",
       " 'Abdel',\n",
       " 'Abdul',\n",
       " 'Abdulkarim',\n",
       " 'Abdullah',\n",
       " 'Abe',\n",
       " 'Abel',\n",
       " 'Abelard',\n",
       " 'Abner',\n",
       " 'Abraham',\n",
       " 'Abram',\n",
       " 'Ace',\n",
       " 'Adair',\n",
       " 'Adam',\n",
       " 'Adams',\n",
       " 'Addie',\n",
       " 'Adger',\n",
       " 'Aditya',\n",
       " 'Adlai',\n",
       " 'Adnan',\n",
       " 'Adolf',\n",
       " 'Adolfo',\n",
       " 'Adolph',\n",
       " 'Adolphe',\n",
       " 'Adolpho',\n",
       " 'Adolphus',\n",
       " 'Adrian',\n",
       " 'Adrick',\n",
       " 'Adrien',\n",
       " 'Agamemnon',\n",
       " 'Aguinaldo',\n",
       " 'Aguste',\n",
       " 'Agustin',\n",
       " 'Aharon',\n",
       " 'Ahmad',\n",
       " 'Ahmed',\n",
       " 'Ahmet',\n",
       " 'Ajai',\n",
       " 'Ajay',\n",
       " 'Al',\n",
       " 'Alaa',\n",
       " 'Alain',\n",
       " 'Alan',\n",
       " 'Alasdair',\n",
       " 'Alastair',\n",
       " 'Albatros',\n",
       " 'Albert',\n",
       " 'Alberto',\n",
       " 'Albrecht',\n",
       " 'Alden',\n",
       " 'Aldis',\n",
       " 'Aldo',\n",
       " 'Aldric',\n",
       " 'Aldrich',\n",
       " 'Aldus',\n",
       " 'Aldwin',\n",
       " 'Alec',\n",
       " 'Aleck',\n",
       " 'Alejandro',\n",
       " 'Aleks',\n",
       " 'Aleksandrs',\n",
       " 'Alessandro',\n",
       " 'Alex',\n",
       " 'Alexander',\n",
       " 'Alexei',\n",
       " 'Alexis',\n",
       " 'Alf',\n",
       " 'Alfie',\n",
       " 'Alfonse',\n",
       " 'Alfonso',\n",
       " 'Alfonzo',\n",
       " 'Alford',\n",
       " 'Alfred',\n",
       " 'Alfredo',\n",
       " 'Algernon',\n",
       " 'Ali',\n",
       " 'Alic',\n",
       " 'Alister',\n",
       " 'Alix',\n",
       " 'Allah',\n",
       " 'Allan',\n",
       " 'Allen',\n",
       " 'Alley',\n",
       " 'Allie',\n",
       " 'Allin',\n",
       " 'Allyn',\n",
       " 'Alonso',\n",
       " 'Alonzo',\n",
       " 'Aloysius',\n",
       " 'Alphonse',\n",
       " 'Alphonso',\n",
       " 'Alston',\n",
       " 'Alton',\n",
       " 'Alvin',\n",
       " 'Alwin',\n",
       " 'Amadeus',\n",
       " 'Ambros',\n",
       " 'Ambrose',\n",
       " 'Ambrosi',\n",
       " 'Ambrosio',\n",
       " 'Ambrosius',\n",
       " 'Amery',\n",
       " 'Amory',\n",
       " 'Amos',\n",
       " 'Anatol',\n",
       " 'Anatole',\n",
       " 'Anatollo',\n",
       " 'Anatoly',\n",
       " 'Anders',\n",
       " 'Andie',\n",
       " 'Andonis',\n",
       " 'Andre',\n",
       " 'Andrea',\n",
       " 'Andreas',\n",
       " 'Andrej',\n",
       " 'Andres',\n",
       " 'Andrew',\n",
       " 'Andrey',\n",
       " 'Andri',\n",
       " 'Andros',\n",
       " 'Andrus',\n",
       " 'Andrzej',\n",
       " 'Andy',\n",
       " 'Angel',\n",
       " 'Angelico',\n",
       " 'Angelo',\n",
       " 'Angie',\n",
       " 'Angus',\n",
       " 'Ansel',\n",
       " 'Ansell',\n",
       " 'Anselm',\n",
       " 'Anson',\n",
       " 'Anthony',\n",
       " 'Antin',\n",
       " 'Antoine',\n",
       " 'Anton',\n",
       " 'Antone',\n",
       " 'Antoni',\n",
       " 'Antonin',\n",
       " 'Antonino',\n",
       " 'Antonio',\n",
       " 'Antonius',\n",
       " 'Antony',\n",
       " 'Anurag',\n",
       " 'Apollo',\n",
       " 'Apostolos',\n",
       " 'Aram',\n",
       " 'Archibald',\n",
       " 'Archibold',\n",
       " 'Archie',\n",
       " 'Archon',\n",
       " 'Archy',\n",
       " 'Arel',\n",
       " 'Ari',\n",
       " 'Arie',\n",
       " 'Ariel',\n",
       " 'Aristotle',\n",
       " 'Arlo',\n",
       " 'Armand',\n",
       " 'Armando',\n",
       " 'Armond',\n",
       " 'Armstrong',\n",
       " 'Arne',\n",
       " 'Arnie',\n",
       " 'Arnold',\n",
       " 'Arnoldo',\n",
       " 'Aron',\n",
       " 'Arron',\n",
       " 'Art',\n",
       " 'Arther',\n",
       " 'Arthur',\n",
       " 'Artie',\n",
       " 'Artur',\n",
       " 'Arturo',\n",
       " 'Arvie',\n",
       " 'Arvin',\n",
       " 'Arvind',\n",
       " 'Arvy',\n",
       " 'Ash',\n",
       " 'Ashby',\n",
       " 'Ashish',\n",
       " 'Ashley',\n",
       " 'Ashton',\n",
       " 'Aub',\n",
       " 'Aube',\n",
       " 'Aubert',\n",
       " 'Aubrey',\n",
       " 'Augie',\n",
       " 'August',\n",
       " 'Augustin',\n",
       " 'Augustine',\n",
       " 'Augusto',\n",
       " 'Augustus',\n",
       " 'Austen',\n",
       " 'Austin',\n",
       " 'Ave',\n",
       " 'Averell',\n",
       " 'Averil',\n",
       " 'Averill',\n",
       " 'Avery',\n",
       " 'Avi',\n",
       " 'Avraham',\n",
       " 'Avram',\n",
       " 'Avrom',\n",
       " 'Axel',\n",
       " 'Aylmer',\n",
       " 'Aziz',\n",
       " 'Bailey',\n",
       " 'Bailie',\n",
       " 'Baillie',\n",
       " 'Baily',\n",
       " 'Baird',\n",
       " 'Baldwin',\n",
       " 'Bancroft',\n",
       " 'Barbabas',\n",
       " 'Barclay',\n",
       " 'Bard',\n",
       " 'Barde',\n",
       " 'Barn',\n",
       " 'Barnabas',\n",
       " 'Barnabe',\n",
       " 'Barnaby',\n",
       " 'Barnard',\n",
       " 'Barnebas',\n",
       " 'Barnett',\n",
       " 'Barney',\n",
       " 'Barnie',\n",
       " 'Barny',\n",
       " 'Baron',\n",
       " 'Barr',\n",
       " 'Barret',\n",
       " 'Barrett',\n",
       " 'Barri',\n",
       " 'Barrie',\n",
       " 'Barris',\n",
       " 'Barron',\n",
       " 'Barry',\n",
       " 'Bart',\n",
       " 'Bartel',\n",
       " 'Barth',\n",
       " 'Barthel',\n",
       " 'Bartholemy',\n",
       " 'Bartholomeo',\n",
       " 'Bartholomeus',\n",
       " 'Bartholomew',\n",
       " 'Bartie',\n",
       " 'Bartlet',\n",
       " 'Bartlett',\n",
       " 'Bartolemo',\n",
       " 'Bartolomei',\n",
       " 'Bartolomeo',\n",
       " 'Barton',\n",
       " 'Barty',\n",
       " 'Bary',\n",
       " 'Basil',\n",
       " 'Batholomew',\n",
       " 'Baxter',\n",
       " 'Bay',\n",
       " 'Bayard',\n",
       " 'Beale',\n",
       " 'Bealle',\n",
       " 'Bear',\n",
       " 'Bearnard',\n",
       " 'Beau',\n",
       " 'Beaufort',\n",
       " 'Beauregard',\n",
       " 'Beck',\n",
       " 'Bela',\n",
       " 'Ben',\n",
       " 'Benedict',\n",
       " 'Bengt',\n",
       " 'Benito',\n",
       " 'Benjamen',\n",
       " 'Benjamin',\n",
       " 'Benji',\n",
       " 'Benjie',\n",
       " 'Benjy',\n",
       " 'Benn',\n",
       " 'Bennet',\n",
       " 'Bennett',\n",
       " 'Bennie',\n",
       " 'Benny',\n",
       " 'Benson',\n",
       " 'Bentley',\n",
       " 'Benton',\n",
       " 'Beowulf',\n",
       " 'Berchtold',\n",
       " 'Berk',\n",
       " 'Berke',\n",
       " 'Berkeley',\n",
       " 'Berkie',\n",
       " 'Berkley',\n",
       " 'Bernard',\n",
       " 'Bernardo',\n",
       " 'Bernd',\n",
       " 'Bernhard',\n",
       " 'Bernie',\n",
       " 'Bert',\n",
       " 'Bertie',\n",
       " 'Bertram',\n",
       " 'Bertrand',\n",
       " 'Bharat',\n",
       " 'Biff',\n",
       " 'Bill',\n",
       " 'Billie',\n",
       " 'Billy',\n",
       " 'Bing',\n",
       " 'Binky',\n",
       " 'Bishop',\n",
       " 'Bjorn',\n",
       " 'Bjorne',\n",
       " 'Blaine',\n",
       " 'Blair',\n",
       " 'Blake',\n",
       " 'Blare',\n",
       " 'Blayne',\n",
       " 'Bo',\n",
       " 'Bob',\n",
       " 'Bobbie',\n",
       " 'Bobby',\n",
       " 'Bogart',\n",
       " 'Bogdan',\n",
       " 'Boniface',\n",
       " 'Boris',\n",
       " 'Boyce',\n",
       " 'Boyd',\n",
       " 'Brad',\n",
       " 'Braden',\n",
       " 'Bradford',\n",
       " 'Bradley',\n",
       " 'Bradly',\n",
       " 'Brady',\n",
       " 'Brandon',\n",
       " 'Brandy',\n",
       " 'Brant',\n",
       " 'Brendan',\n",
       " 'Brent',\n",
       " 'Bret',\n",
       " 'Brett',\n",
       " 'Brewer',\n",
       " 'Brewster',\n",
       " 'Brian',\n",
       " 'Brice',\n",
       " 'Briggs',\n",
       " 'Brinkley',\n",
       " 'Britt',\n",
       " 'Brock',\n",
       " 'Broddie',\n",
       " 'Broddy',\n",
       " 'Broderic',\n",
       " 'Broderick',\n",
       " 'Brodie',\n",
       " 'Brody',\n",
       " 'Bronson',\n",
       " 'Brook',\n",
       " 'Brooke',\n",
       " 'Brooks',\n",
       " 'Bruce',\n",
       " 'Bruno',\n",
       " 'Bryan',\n",
       " 'Bryant',\n",
       " 'Bryce',\n",
       " 'Bryn',\n",
       " 'Bryon',\n",
       " 'Bubba',\n",
       " 'Buck',\n",
       " 'Bucky',\n",
       " 'Bud',\n",
       " 'Buddy',\n",
       " 'Burgess',\n",
       " 'Burke',\n",
       " 'Burl',\n",
       " 'Burnaby',\n",
       " 'Burt',\n",
       " 'Burton',\n",
       " 'Buster',\n",
       " 'Butch',\n",
       " 'Butler',\n",
       " 'Byram',\n",
       " 'Byron',\n",
       " 'Caesar',\n",
       " 'Cain',\n",
       " 'Cal',\n",
       " 'Caldwell',\n",
       " 'Caleb',\n",
       " 'Calhoun',\n",
       " 'Calvin',\n",
       " 'Cam',\n",
       " 'Cameron',\n",
       " 'Cammy',\n",
       " 'Carey',\n",
       " 'Carl',\n",
       " 'Carleigh',\n",
       " 'Carlie',\n",
       " 'Carlin',\n",
       " 'Carlo',\n",
       " 'Carlos',\n",
       " 'Carlton',\n",
       " 'Carlyle',\n",
       " 'Carmine',\n",
       " 'Carroll',\n",
       " 'Carson',\n",
       " 'Carsten',\n",
       " 'Carter',\n",
       " 'Cary',\n",
       " 'Caryl',\n",
       " 'Case',\n",
       " 'Casey',\n",
       " 'Caspar',\n",
       " 'Casper',\n",
       " 'Cass',\n",
       " 'Cat',\n",
       " 'Cecil',\n",
       " 'Cesar',\n",
       " 'Chad',\n",
       " 'Chadd',\n",
       " 'Chaddie',\n",
       " 'Chaddy',\n",
       " 'Chadwick',\n",
       " 'Chaim',\n",
       " 'Chalmers',\n",
       " 'Chan',\n",
       " 'Chance',\n",
       " 'Chancey',\n",
       " 'Chanderjit',\n",
       " 'Chandler',\n",
       " 'Chane',\n",
       " 'Chariot',\n",
       " 'Charles',\n",
       " 'Charleton',\n",
       " 'Charley',\n",
       " 'Charlie',\n",
       " 'Charlton',\n",
       " 'Chas',\n",
       " 'Chase',\n",
       " 'Chaunce',\n",
       " 'Chauncey',\n",
       " 'Che',\n",
       " 'Chelton',\n",
       " 'Chen',\n",
       " 'Chester',\n",
       " 'Cheston',\n",
       " 'Chet',\n",
       " 'Chev',\n",
       " 'Chevalier',\n",
       " 'Chevy',\n",
       " 'Chip',\n",
       " 'Chris',\n",
       " 'Chrissy',\n",
       " 'Christ',\n",
       " 'Christian',\n",
       " 'Christiano',\n",
       " 'Christie',\n",
       " 'Christof',\n",
       " 'Christofer',\n",
       " 'Christoph',\n",
       " 'Christophe',\n",
       " 'Christopher',\n",
       " 'Christorpher',\n",
       " 'Christos',\n",
       " 'Christy',\n",
       " 'Chrisy',\n",
       " 'Chuck',\n",
       " 'Churchill',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'Clancy',\n",
       " 'Clarance',\n",
       " 'Clare',\n",
       " 'Clarence',\n",
       " 'Clark',\n",
       " 'Clarke',\n",
       " 'Claude',\n",
       " 'Claudio',\n",
       " 'Claudius',\n",
       " 'Claus',\n",
       " 'Clay',\n",
       " 'Clayborn',\n",
       " 'Clayborne',\n",
       " 'Claybourne',\n",
       " 'Clayton',\n",
       " 'Cleland',\n",
       " 'Clem',\n",
       " 'Clemens',\n",
       " 'Clement',\n",
       " 'Clemente',\n",
       " 'Clemmie',\n",
       " 'Cletus',\n",
       " 'Cleveland',\n",
       " 'Cliff',\n",
       " 'Clifford',\n",
       " 'Clifton',\n",
       " 'Clint',\n",
       " 'Clinten',\n",
       " 'Clinton',\n",
       " 'Clive',\n",
       " 'Clyde',\n",
       " 'Cob',\n",
       " 'Cobb',\n",
       " 'Cobbie',\n",
       " 'Cobby',\n",
       " 'Cody',\n",
       " 'Colbert',\n",
       " 'Cole',\n",
       " 'Coleman',\n",
       " 'Colin',\n",
       " 'Collin',\n",
       " 'Collins',\n",
       " 'Conan',\n",
       " 'Connie',\n",
       " 'Connolly',\n",
       " 'Connor',\n",
       " 'Conrad',\n",
       " 'Conroy',\n",
       " 'Constantin',\n",
       " 'Constantine',\n",
       " 'Constantinos',\n",
       " 'Conway',\n",
       " 'Cooper',\n",
       " 'Corbin',\n",
       " 'Corby',\n",
       " 'Corey',\n",
       " 'Corky',\n",
       " 'Cornelius',\n",
       " 'Cornellis',\n",
       " 'Corrie',\n",
       " 'Cortese',\n",
       " 'Corwin',\n",
       " 'Cory',\n",
       " 'Cosmo',\n",
       " 'Costa',\n",
       " 'Courtney',\n",
       " 'Craig',\n",
       " 'Crawford',\n",
       " 'Creighton',\n",
       " 'Cris',\n",
       " 'Cristopher',\n",
       " 'Curt',\n",
       " 'Curtice',\n",
       " 'Curtis',\n",
       " 'Cy',\n",
       " 'Cyril',\n",
       " 'Cyrill',\n",
       " 'Cyrille',\n",
       " 'Cyrillus',\n",
       " 'Cyrus',\n",
       " 'Dabney',\n",
       " 'Daffy',\n",
       " 'Dale',\n",
       " 'Dallas',\n",
       " 'Dalton',\n",
       " 'Damian',\n",
       " 'Damien',\n",
       " 'Damon',\n",
       " 'Dan',\n",
       " 'Dana',\n",
       " 'Dane',\n",
       " 'Dani',\n",
       " 'Danie',\n",
       " 'Daniel',\n",
       " 'Dannie',\n",
       " 'Danny',\n",
       " 'Dante',\n",
       " 'Darby',\n",
       " 'Darcy',\n",
       " 'Daren',\n",
       " 'Darian',\n",
       " 'Darien',\n",
       " 'Darin',\n",
       " 'Dario',\n",
       " 'Darius',\n",
       " 'Darrel',\n",
       " 'Darrell',\n",
       " 'Darren',\n",
       " 'Darrick',\n",
       " 'Darrin',\n",
       " 'Darryl',\n",
       " 'Darth',\n",
       " 'Darwin',\n",
       " 'Daryl',\n",
       " 'Daryle',\n",
       " 'Dave',\n",
       " 'Davey',\n",
       " 'David',\n",
       " 'Davidde',\n",
       " 'Davide',\n",
       " 'Davidson',\n",
       " 'Davie',\n",
       " 'Davin',\n",
       " 'Davis',\n",
       " 'Davon',\n",
       " 'Davoud',\n",
       " 'Davy',\n",
       " 'Dawson',\n",
       " 'Dean',\n",
       " 'Deane',\n",
       " 'Del',\n",
       " 'Delbert',\n",
       " 'Dell',\n",
       " 'Delmar',\n",
       " 'Demetre',\n",
       " 'Demetri',\n",
       " 'Demetris',\n",
       " 'Demetrius',\n",
       " 'Demosthenis',\n",
       " 'Denis',\n",
       " 'Dennie',\n",
       " 'Dennis',\n",
       " 'Denny',\n",
       " 'Derby',\n",
       " 'Derek',\n",
       " 'Derick',\n",
       " 'Derk',\n",
       " 'Derrek',\n",
       " 'Derrick',\n",
       " 'Derrin',\n",
       " 'Derrol',\n",
       " 'Derron',\n",
       " 'Deryl',\n",
       " 'Desmond',\n",
       " 'Desmund',\n",
       " 'Devin',\n",
       " 'Devon',\n",
       " 'Dewey',\n",
       " 'Dewitt',\n",
       " 'Dexter',\n",
       " 'Dick',\n",
       " 'Dickey',\n",
       " 'Dickie',\n",
       " 'Diego',\n",
       " 'Dieter',\n",
       " 'Dietrich',\n",
       " 'Dillon',\n",
       " 'Dimitri',\n",
       " 'Dimitrios',\n",
       " 'Dimitris',\n",
       " 'Dimitrou',\n",
       " 'Dimitry',\n",
       " 'Dino',\n",
       " 'Dion',\n",
       " 'Dionis',\n",
       " 'Dionysus',\n",
       " 'Dirk',\n",
       " 'Dmitri',\n",
       " 'Dom',\n",
       " 'Domenic',\n",
       " 'Domenico',\n",
       " 'Dominic',\n",
       " 'Dominick',\n",
       " 'Dominique',\n",
       " 'Don',\n",
       " 'Donal',\n",
       " 'Donald',\n",
       " 'Donn',\n",
       " 'Donnie',\n",
       " 'Donny',\n",
       " 'Donovan',\n",
       " 'Dorian',\n",
       " 'Dory',\n",
       " 'Doug',\n",
       " 'Douggie',\n",
       " 'Dougie',\n",
       " 'Douglas',\n",
       " 'Douglass',\n",
       " 'Douglis',\n",
       " 'Dov',\n",
       " 'Doyle',\n",
       " 'Drake',\n",
       " 'Drew',\n",
       " 'Dru',\n",
       " 'Dryke',\n",
       " 'Duane',\n",
       " 'Dudley',\n",
       " 'Duffie',\n",
       " 'Duffy',\n",
       " 'Dugan',\n",
       " 'Duke',\n",
       " 'Dunc',\n",
       " 'Duncan',\n",
       " 'Dunstan',\n",
       " 'Durand',\n",
       " 'Durant',\n",
       " 'Durante',\n",
       " 'Durward',\n",
       " 'Dustin',\n",
       " 'Dwain',\n",
       " 'Dwaine',\n",
       " 'Dwane',\n",
       " 'Dwayne',\n",
       " 'Dwight',\n",
       " 'Dylan',\n",
       " 'Dyson',\n",
       " 'Earl',\n",
       " 'Earle',\n",
       " 'Easton',\n",
       " 'Eben',\n",
       " 'Ebeneser',\n",
       " 'Ebenezer',\n",
       " 'Eberhard',\n",
       " 'Ed',\n",
       " 'Eddie',\n",
       " 'Eddy',\n",
       " 'Edgar',\n",
       " 'Edgardo',\n",
       " 'Edie',\n",
       " 'Edmond',\n",
       " 'Edmund',\n",
       " 'Edouard',\n",
       " 'Edsel',\n",
       " 'Eduard',\n",
       " 'Eduardo',\n",
       " 'Edward',\n",
       " 'Edwin',\n",
       " 'Efram',\n",
       " 'Egbert',\n",
       " 'Ehud',\n",
       " 'Elbert',\n",
       " 'Elden',\n",
       " 'Eldon',\n",
       " 'Eli',\n",
       " 'Elias',\n",
       " 'Elihu',\n",
       " 'Elijah',\n",
       " 'Eliot',\n",
       " 'Eliott',\n",
       " 'Elisha',\n",
       " 'Elliot',\n",
       " 'Elliott',\n",
       " 'Ellis',\n",
       " 'Ellsworth',\n",
       " 'Ellwood',\n",
       " 'Elmer',\n",
       " 'Elmore',\n",
       " 'Elnar',\n",
       " 'Elric',\n",
       " 'Elroy',\n",
       " 'Elton',\n",
       " 'Elvin',\n",
       " 'Elvis',\n",
       " 'Elwin',\n",
       " 'Elwood',\n",
       " 'Elwyn',\n",
       " 'Ely',\n",
       " 'Emanuel',\n",
       " 'Emerson',\n",
       " 'Emery',\n",
       " 'Emil',\n",
       " 'Emile',\n",
       " 'Emilio',\n",
       " 'Emmanuel',\n",
       " 'Emmery',\n",
       " 'Emmet',\n",
       " 'Emmett',\n",
       " 'Emmit',\n",
       " 'Emmott',\n",
       " 'Emmy',\n",
       " 'Emory',\n",
       " 'Ender',\n",
       " 'Engelbart',\n",
       " 'Engelbert',\n",
       " 'Englebart',\n",
       " 'Englebert',\n",
       " 'Enoch',\n",
       " 'Enrico',\n",
       " 'Enrique',\n",
       " 'Ephraim',\n",
       " 'Ephram',\n",
       " 'Ephrayim',\n",
       " 'Ephrem',\n",
       " 'Er',\n",
       " 'Erasmus',\n",
       " 'Erastus',\n",
       " 'Erek',\n",
       " 'Erhard',\n",
       " 'Erhart',\n",
       " 'Eric',\n",
       " 'Erich',\n",
       " 'Erick',\n",
       " 'Erik',\n",
       " 'Erin',\n",
       " 'Erl',\n",
       " 'Ernest',\n",
       " 'Ernesto',\n",
       " 'Ernie',\n",
       " 'Ernst',\n",
       " 'Erny',\n",
       " 'Errol',\n",
       " 'Ervin',\n",
       " 'Erwin',\n",
       " 'Esau',\n",
       " 'Esme',\n",
       " 'Esteban',\n",
       " 'Ethan',\n",
       " 'Ethelbert',\n",
       " 'Ethelred',\n",
       " 'Etienne',\n",
       " 'Euclid',\n",
       " 'Eugen',\n",
       " 'Eugene',\n",
       " 'Eustace',\n",
       " 'Ev',\n",
       " 'Evan',\n",
       " 'Evelyn',\n",
       " 'Everard',\n",
       " 'Everett',\n",
       " 'Ewan',\n",
       " 'Ewart',\n",
       " 'Ez',\n",
       " 'Ezechiel',\n",
       " 'Ezekiel',\n",
       " 'Ezra',\n",
       " 'Fabian',\n",
       " 'Fabio',\n",
       " 'Fairfax',\n",
       " 'Farley',\n",
       " 'Fazeel',\n",
       " 'Federico',\n",
       " 'Felice',\n",
       " 'Felicio',\n",
       " 'Felipe',\n",
       " 'Felix',\n",
       " 'Ferd',\n",
       " 'Ferdie',\n",
       " 'Ferdinand',\n",
       " 'Ferdy',\n",
       " 'Fergus',\n",
       " 'Ferguson',\n",
       " 'Ferinand',\n",
       " 'Fernando',\n",
       " 'Fidel',\n",
       " 'Filbert',\n",
       " 'Filip',\n",
       " 'Filipe',\n",
       " 'Filmore',\n",
       " 'Finley',\n",
       " 'Finn',\n",
       " 'Fitz',\n",
       " 'Fitzgerald',\n",
       " 'Flem',\n",
       " 'Fleming',\n",
       " 'Flemming',\n",
       " 'Fletch',\n",
       " 'Fletcher',\n",
       " 'Flin',\n",
       " 'Flinn',\n",
       " 'Flint',\n",
       " 'Flipper',\n",
       " 'Florian',\n",
       " 'Floyd',\n",
       " 'Flynn',\n",
       " 'Fons',\n",
       " 'Fonsie',\n",
       " 'Fonz',\n",
       " 'Fonzie',\n",
       " 'Forbes',\n",
       " 'Ford',\n",
       " 'Forest',\n",
       " 'Forester',\n",
       " 'Forrest',\n",
       " 'Forrester',\n",
       " 'Forster',\n",
       " 'Foster',\n",
       " 'Fowler',\n",
       " 'Fox',\n",
       " 'Fran',\n",
       " 'Francesco',\n",
       " 'Francis',\n",
       " 'Francisco',\n",
       " 'Francois',\n",
       " 'Frank',\n",
       " 'Frankie',\n",
       " 'Franklin',\n",
       " 'Franklyn',\n",
       " 'Franky',\n",
       " 'Frans',\n",
       " 'Franz',\n",
       " 'Fraser',\n",
       " 'Frazier',\n",
       " 'Fred',\n",
       " 'Freddie',\n",
       " 'Freddy',\n",
       " 'Frederic',\n",
       " 'Frederich',\n",
       " 'Frederick',\n",
       " 'Frederico',\n",
       " 'Frederik',\n",
       " 'Fredric',\n",
       " 'Fredrick',\n",
       " 'Freeman',\n",
       " 'Freemon',\n",
       " 'Fremont',\n",
       " 'French',\n",
       " 'Friedric',\n",
       " 'Friedrich',\n",
       " 'Friedrick',\n",
       " 'Fritz',\n",
       " 'Fulton',\n",
       " 'Fyodor',\n",
       " 'Gabe',\n",
       " 'Gabriel',\n",
       " 'Gabriele',\n",
       " 'Gabriell',\n",
       " 'Gabriello',\n",
       " 'Gail',\n",
       " 'Gale',\n",
       " 'Galen',\n",
       " 'Gallagher',\n",
       " 'Gamaliel',\n",
       " 'Garcia',\n",
       " 'Garcon',\n",
       " 'Gardener',\n",
       " 'Gardiner',\n",
       " 'Gardner',\n",
       " 'Garey',\n",
       " 'Garfield',\n",
       " 'Garfinkel',\n",
       " 'Garold',\n",
       " 'Garp',\n",
       " 'Garret',\n",
       " 'Garrett',\n",
       " 'Garrot',\n",
       " 'Garrott',\n",
       " 'Garry',\n",
       " 'Garth',\n",
       " 'Garv',\n",
       " 'Garvey',\n",
       " 'Garvin',\n",
       " 'Garvy',\n",
       " 'Garwin',\n",
       " 'Garwood',\n",
       " 'Gary',\n",
       " 'Gaspar',\n",
       " 'Gasper',\n",
       " 'Gaston',\n",
       " 'Gav',\n",
       " 'Gaven',\n",
       " 'Gavin',\n",
       " 'Gavriel',\n",
       " 'Gay',\n",
       " 'Gayle',\n",
       " 'Gearard',\n",
       " 'Gene',\n",
       " 'Geo',\n",
       " 'Geof',\n",
       " 'Geoff',\n",
       " 'Geoffrey',\n",
       " 'Geoffry',\n",
       " 'Georg',\n",
       " 'George',\n",
       " 'Georges',\n",
       " 'Georgia',\n",
       " 'Georgie',\n",
       " 'Georgy',\n",
       " 'Gerald',\n",
       " 'Geraldo',\n",
       " 'Gerard',\n",
       " 'Gere',\n",
       " 'Gerhard',\n",
       " 'Gerhardt',\n",
       " 'Geri',\n",
       " 'Germaine',\n",
       " 'Gerold',\n",
       " 'Gerome',\n",
       " 'Gerrard',\n",
       " 'Gerri',\n",
       " 'Gerrit',\n",
       " 'Gerry',\n",
       " 'Gershom',\n",
       " 'Gershon',\n",
       " 'Giacomo',\n",
       " 'Gian',\n",
       " 'Giancarlo',\n",
       " 'Giavani',\n",
       " 'Gibb',\n",
       " 'Gideon',\n",
       " 'Giff',\n",
       " 'Giffard',\n",
       " 'Giffer',\n",
       " 'Giffie',\n",
       " 'Gifford',\n",
       " 'Giffy',\n",
       " 'Gil',\n",
       " 'Gilbert',\n",
       " 'Gilberto',\n",
       " 'Gilburt',\n",
       " 'Giles',\n",
       " 'Gill',\n",
       " 'Gilles',\n",
       " 'Ginger',\n",
       " 'Gino',\n",
       " 'Giordano',\n",
       " 'Giorgi',\n",
       " 'Giorgio',\n",
       " 'Giovanne',\n",
       " ...]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words('male.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract last N letters from the input word and that will act as our \"feature\"\n",
    "def extract_features(word, N=2):\n",
    "    last_n_letters = word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': 'y'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(\"Marky\", N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data using labeled names available in NLTK\n",
    "male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "data = (male_list + female_list)\n",
    "\n",
    "# Seed the random number generator\n",
    "random.seed(5)\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Create test data\n",
    "input_names = ['Alexander', 'Danielle', 'David', 'Cheryl', 'Robert']\n",
    "\n",
    "# Define the number of samples used for train and test\n",
    "num_train = int(0.8 * len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Dasi', 'female')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy używać ostatnich N znaków jako wektora cech do przewidywania płci. Będziemy zmieniać ten parametr, aby zobaczyć, jak zmienia się wydajność. W takim przypadku przejdziemy od 1 do 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of end letters: 1\n",
      "Accuracy = 74.7%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> male\n",
      "Robert ==> male\n",
      "\n",
      "Number of end letters: 2\n",
      "Accuracy = 78.79%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "Robert ==> male\n",
      "\n",
      "Number of end letters: 3\n",
      "Accuracy = 77.22%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "Robert ==> male\n",
      "\n",
      "Number of end letters: 4\n",
      "Accuracy = 69.98%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "Robert ==> male\n",
      "\n",
      "Number of end letters: 5\n",
      "Accuracy = 64.63%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "Robert ==> male\n"
     ]
    }
   ],
   "source": [
    "# Iterate through different lengths to compare the accuracy\n",
    "for i in range(1, 6):\n",
    "    print('\\nNumber of end letters:', i)\n",
    "    features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
    "    train_data, test_data = features[:num_train], features[num_train:]\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    # Compute the accuracy of the classifier \n",
    "    accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
    "    print('Accuracy = ' + str(accuracy) + '%')\n",
    "\n",
    "    # Predict outputs for input names using the trained classifier model\n",
    "    for name in input_names:\n",
    "        print(name, '==>', classifier.classify(extract_features(name, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowanie analizatora nastrojów\n",
    "\n",
    "**Analiza sentymentu** to proces określania nastroju danego fragmentu tekstu. Na przykład może służyć do określenia, czy recenzja filmu jest pozytywna, czy negatywna. Jest to jedno z najpopularniejszych zastosowań przetwarzania języka naturalnego. W zależności od problemu możemy dodać więcej kategorii. **Ta technika jest zwykle używana, aby zorientować się, jak ludzie myślą o określonym produkcie, marce lub temacie. Jest często używany do analizowania kampanii marketingowych, sondaży, obecności w mediach społecznościowych, recenzji produktów w witrynach e-commerce i tak dalej.** Zobaczmy, jak określić sentyment recenzji filmu.\n",
    "\n",
    "Do zbudowania tego klasyfikatora użyjemy klasyfikatora Naive Bayes. Najpierw musimy wyodrębnić wszystkie unikalne słowa z tekstu. Klasyfikator NLTK wymaga uporządkowania tych danych w postaci słownika, aby mógł je przyswoić. Po podzieleniu danych tekstowych na zestawy danych uczących i testujących, wyszkolimy klasyfikator Naive Bayes, aby klasyfikował recenzje na pozytywne i negatywne. Wydrukujemy również najważniejsze słowa informacyjne, aby wskazać pozytywne i negatywne recenzje. Ta informacja jest interesująca, ponieważ mówi nam, jakie słowa są używane do określenia różnych reakcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Extract features from the input list of words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos/cv000_29590.txt',\n",
       " 'pos/cv001_18431.txt',\n",
       " 'pos/cv002_15918.txt',\n",
       " 'pos/cv003_11664.txt',\n",
       " 'pos/cv004_11636.txt',\n",
       " 'pos/cv005_29443.txt',\n",
       " 'pos/cv006_15448.txt',\n",
       " 'pos/cv007_4968.txt',\n",
       " 'pos/cv008_29435.txt',\n",
       " 'pos/cv009_29592.txt',\n",
       " 'pos/cv010_29198.txt',\n",
       " 'pos/cv011_12166.txt',\n",
       " 'pos/cv012_29576.txt',\n",
       " 'pos/cv013_10159.txt',\n",
       " 'pos/cv014_13924.txt',\n",
       " 'pos/cv015_29439.txt',\n",
       " 'pos/cv016_4659.txt',\n",
       " 'pos/cv017_22464.txt',\n",
       " 'pos/cv018_20137.txt',\n",
       " 'pos/cv019_14482.txt',\n",
       " 'pos/cv020_8825.txt',\n",
       " 'pos/cv021_15838.txt',\n",
       " 'pos/cv022_12864.txt',\n",
       " 'pos/cv023_12672.txt',\n",
       " 'pos/cv024_6778.txt',\n",
       " 'pos/cv025_3108.txt',\n",
       " 'pos/cv026_29325.txt',\n",
       " 'pos/cv027_25219.txt',\n",
       " 'pos/cv028_26746.txt',\n",
       " 'pos/cv029_18643.txt',\n",
       " 'pos/cv030_21593.txt',\n",
       " 'pos/cv031_18452.txt',\n",
       " 'pos/cv032_22550.txt',\n",
       " 'pos/cv033_24444.txt',\n",
       " 'pos/cv034_29647.txt',\n",
       " 'pos/cv035_3954.txt',\n",
       " 'pos/cv036_16831.txt',\n",
       " 'pos/cv037_18510.txt',\n",
       " 'pos/cv038_9749.txt',\n",
       " 'pos/cv039_6170.txt',\n",
       " 'pos/cv040_8276.txt',\n",
       " 'pos/cv041_21113.txt',\n",
       " 'pos/cv042_10982.txt',\n",
       " 'pos/cv043_15013.txt',\n",
       " 'pos/cv044_16969.txt',\n",
       " 'pos/cv045_23923.txt',\n",
       " 'pos/cv046_10188.txt',\n",
       " 'pos/cv047_1754.txt',\n",
       " 'pos/cv048_16828.txt',\n",
       " 'pos/cv049_20471.txt',\n",
       " 'pos/cv050_11175.txt',\n",
       " 'pos/cv051_10306.txt',\n",
       " 'pos/cv052_29378.txt',\n",
       " 'pos/cv053_21822.txt',\n",
       " 'pos/cv054_4230.txt',\n",
       " 'pos/cv055_8338.txt',\n",
       " 'pos/cv056_13133.txt',\n",
       " 'pos/cv057_7453.txt',\n",
       " 'pos/cv058_8025.txt',\n",
       " 'pos/cv059_28885.txt',\n",
       " 'pos/cv060_10844.txt',\n",
       " 'pos/cv061_8837.txt',\n",
       " 'pos/cv062_23115.txt',\n",
       " 'pos/cv063_28997.txt',\n",
       " 'pos/cv064_24576.txt',\n",
       " 'pos/cv065_15248.txt',\n",
       " 'pos/cv066_10821.txt',\n",
       " 'pos/cv067_19774.txt',\n",
       " 'pos/cv068_13400.txt',\n",
       " 'pos/cv069_10801.txt',\n",
       " 'pos/cv070_12289.txt',\n",
       " 'pos/cv071_12095.txt',\n",
       " 'pos/cv072_6169.txt',\n",
       " 'pos/cv073_21785.txt',\n",
       " 'pos/cv074_6875.txt',\n",
       " 'pos/cv075_6500.txt',\n",
       " 'pos/cv076_24945.txt',\n",
       " 'pos/cv077_22138.txt',\n",
       " 'pos/cv078_14730.txt',\n",
       " 'pos/cv079_11933.txt',\n",
       " 'pos/cv080_13465.txt',\n",
       " 'pos/cv081_16582.txt',\n",
       " 'pos/cv082_11080.txt',\n",
       " 'pos/cv083_24234.txt',\n",
       " 'pos/cv084_13566.txt',\n",
       " 'pos/cv085_1381.txt',\n",
       " 'pos/cv086_18371.txt',\n",
       " 'pos/cv087_1989.txt',\n",
       " 'pos/cv088_24113.txt',\n",
       " 'pos/cv089_11418.txt',\n",
       " 'pos/cv090_0042.txt',\n",
       " 'pos/cv091_7400.txt',\n",
       " 'pos/cv092_28017.txt',\n",
       " 'pos/cv093_13951.txt',\n",
       " 'pos/cv094_27889.txt',\n",
       " 'pos/cv095_28892.txt',\n",
       " 'pos/cv096_11474.txt',\n",
       " 'pos/cv097_24970.txt',\n",
       " 'pos/cv098_15435.txt',\n",
       " 'pos/cv099_10534.txt',\n",
       " 'pos/cv100_11528.txt',\n",
       " 'pos/cv101_10175.txt',\n",
       " 'pos/cv102_7846.txt',\n",
       " 'pos/cv103_11021.txt',\n",
       " 'pos/cv104_18134.txt',\n",
       " 'pos/cv105_17990.txt',\n",
       " 'pos/cv106_16807.txt',\n",
       " 'pos/cv107_24319.txt',\n",
       " 'pos/cv108_15571.txt',\n",
       " 'pos/cv109_21172.txt',\n",
       " 'pos/cv110_27788.txt',\n",
       " 'pos/cv111_11473.txt',\n",
       " 'pos/cv112_11193.txt',\n",
       " 'pos/cv113_23102.txt',\n",
       " 'pos/cv114_18398.txt',\n",
       " 'pos/cv115_25396.txt',\n",
       " 'pos/cv116_28942.txt',\n",
       " 'pos/cv117_24295.txt',\n",
       " 'pos/cv118_28980.txt',\n",
       " 'pos/cv119_9867.txt',\n",
       " 'pos/cv120_4111.txt',\n",
       " 'pos/cv121_17302.txt',\n",
       " 'pos/cv122_7392.txt',\n",
       " 'pos/cv123_11182.txt',\n",
       " 'pos/cv124_4122.txt',\n",
       " 'pos/cv125_9391.txt',\n",
       " 'pos/cv126_28971.txt',\n",
       " 'pos/cv127_14711.txt',\n",
       " 'pos/cv128_29627.txt',\n",
       " 'pos/cv129_16741.txt',\n",
       " 'pos/cv130_17083.txt',\n",
       " 'pos/cv131_10713.txt',\n",
       " 'pos/cv132_5618.txt',\n",
       " 'pos/cv133_16336.txt',\n",
       " 'pos/cv134_22246.txt',\n",
       " 'pos/cv135_11603.txt',\n",
       " 'pos/cv136_11505.txt',\n",
       " 'pos/cv137_15422.txt',\n",
       " 'pos/cv138_12721.txt',\n",
       " 'pos/cv139_12873.txt',\n",
       " 'pos/cv140_7479.txt',\n",
       " 'pos/cv141_15686.txt',\n",
       " 'pos/cv142_22516.txt',\n",
       " 'pos/cv143_19666.txt',\n",
       " 'pos/cv144_5007.txt',\n",
       " 'pos/cv145_11472.txt',\n",
       " 'pos/cv146_18458.txt',\n",
       " 'pos/cv147_21193.txt',\n",
       " 'pos/cv148_16345.txt',\n",
       " 'pos/cv149_15670.txt',\n",
       " 'pos/cv150_12916.txt',\n",
       " 'pos/cv151_15771.txt',\n",
       " 'pos/cv152_8736.txt',\n",
       " 'pos/cv153_10779.txt',\n",
       " 'pos/cv154_9328.txt',\n",
       " 'pos/cv155_7308.txt',\n",
       " 'pos/cv156_10481.txt',\n",
       " 'pos/cv157_29372.txt',\n",
       " 'pos/cv158_10390.txt',\n",
       " 'pos/cv159_29505.txt',\n",
       " 'pos/cv160_10362.txt',\n",
       " 'pos/cv161_11425.txt',\n",
       " 'pos/cv162_10424.txt',\n",
       " 'pos/cv163_10052.txt',\n",
       " 'pos/cv164_22447.txt',\n",
       " 'pos/cv165_22619.txt',\n",
       " 'pos/cv166_11052.txt',\n",
       " 'pos/cv167_16376.txt',\n",
       " 'pos/cv168_7050.txt',\n",
       " 'pos/cv169_23778.txt',\n",
       " 'pos/cv170_3006.txt',\n",
       " 'pos/cv171_13537.txt',\n",
       " 'pos/cv172_11131.txt',\n",
       " 'pos/cv173_4471.txt',\n",
       " 'pos/cv174_9659.txt',\n",
       " 'pos/cv175_6964.txt',\n",
       " 'pos/cv176_12857.txt',\n",
       " 'pos/cv177_10367.txt',\n",
       " 'pos/cv178_12972.txt',\n",
       " 'pos/cv179_9228.txt',\n",
       " 'pos/cv180_16113.txt',\n",
       " 'pos/cv181_14401.txt',\n",
       " 'pos/cv182_7281.txt',\n",
       " 'pos/cv183_18612.txt',\n",
       " 'pos/cv184_2673.txt',\n",
       " 'pos/cv185_28654.txt',\n",
       " 'pos/cv186_2269.txt',\n",
       " 'pos/cv187_12829.txt',\n",
       " 'pos/cv188_19226.txt',\n",
       " 'pos/cv189_22934.txt',\n",
       " 'pos/cv190_27052.txt',\n",
       " 'pos/cv191_29719.txt',\n",
       " 'pos/cv192_14395.txt',\n",
       " 'pos/cv193_5416.txt',\n",
       " 'pos/cv194_12079.txt',\n",
       " 'pos/cv195_14528.txt',\n",
       " 'pos/cv196_29027.txt',\n",
       " 'pos/cv197_29328.txt',\n",
       " 'pos/cv198_18180.txt',\n",
       " 'pos/cv199_9629.txt',\n",
       " 'pos/cv200_2915.txt',\n",
       " 'pos/cv201_6997.txt',\n",
       " 'pos/cv202_10654.txt',\n",
       " 'pos/cv203_17986.txt',\n",
       " 'pos/cv204_8451.txt',\n",
       " 'pos/cv205_9457.txt',\n",
       " 'pos/cv206_14293.txt',\n",
       " 'pos/cv207_29284.txt',\n",
       " 'pos/cv208_9020.txt',\n",
       " 'pos/cv209_29118.txt',\n",
       " 'pos/cv210_9312.txt',\n",
       " 'pos/cv211_9953.txt',\n",
       " 'pos/cv212_10027.txt',\n",
       " 'pos/cv213_18934.txt',\n",
       " 'pos/cv214_12294.txt',\n",
       " 'pos/cv215_22240.txt',\n",
       " 'pos/cv216_18738.txt',\n",
       " 'pos/cv217_28842.txt',\n",
       " 'pos/cv218_24352.txt',\n",
       " 'pos/cv219_18626.txt',\n",
       " 'pos/cv220_29059.txt',\n",
       " 'pos/cv221_2695.txt',\n",
       " 'pos/cv222_17395.txt',\n",
       " 'pos/cv223_29066.txt',\n",
       " 'pos/cv224_17661.txt',\n",
       " 'pos/cv225_29224.txt',\n",
       " 'pos/cv226_2618.txt',\n",
       " 'pos/cv227_24215.txt',\n",
       " 'pos/cv228_5806.txt',\n",
       " 'pos/cv229_13611.txt',\n",
       " 'pos/cv230_7428.txt',\n",
       " 'pos/cv231_10425.txt',\n",
       " 'pos/cv232_14991.txt',\n",
       " 'pos/cv233_15964.txt',\n",
       " 'pos/cv234_20643.txt',\n",
       " 'pos/cv235_10217.txt',\n",
       " 'pos/cv236_11565.txt',\n",
       " 'pos/cv237_19221.txt',\n",
       " 'pos/cv238_12931.txt',\n",
       " 'pos/cv239_3385.txt',\n",
       " 'pos/cv240_14336.txt',\n",
       " 'pos/cv241_23130.txt',\n",
       " 'pos/cv242_10638.txt',\n",
       " 'pos/cv243_20728.txt',\n",
       " 'pos/cv244_21649.txt',\n",
       " 'pos/cv245_8569.txt',\n",
       " 'pos/cv246_28807.txt',\n",
       " 'pos/cv247_13142.txt',\n",
       " 'pos/cv248_13987.txt',\n",
       " 'pos/cv249_11640.txt',\n",
       " 'pos/cv250_25616.txt',\n",
       " 'pos/cv251_22636.txt',\n",
       " 'pos/cv252_23779.txt',\n",
       " 'pos/cv253_10077.txt',\n",
       " 'pos/cv254_6027.txt',\n",
       " 'pos/cv255_13683.txt',\n",
       " 'pos/cv256_14740.txt',\n",
       " 'pos/cv257_10975.txt',\n",
       " 'pos/cv258_5792.txt',\n",
       " 'pos/cv259_10934.txt',\n",
       " 'pos/cv260_13959.txt',\n",
       " 'pos/cv261_10954.txt',\n",
       " 'pos/cv262_12649.txt',\n",
       " 'pos/cv263_19259.txt',\n",
       " 'pos/cv264_12801.txt',\n",
       " 'pos/cv265_10814.txt',\n",
       " 'pos/cv266_25779.txt',\n",
       " 'pos/cv267_14952.txt',\n",
       " 'pos/cv268_18834.txt',\n",
       " 'pos/cv269_21732.txt',\n",
       " 'pos/cv270_6079.txt',\n",
       " 'pos/cv271_13837.txt',\n",
       " 'pos/cv272_18974.txt',\n",
       " 'pos/cv273_29112.txt',\n",
       " 'pos/cv274_25253.txt',\n",
       " 'pos/cv275_28887.txt',\n",
       " 'pos/cv276_15684.txt',\n",
       " 'pos/cv277_19091.txt',\n",
       " 'pos/cv278_13041.txt',\n",
       " 'pos/cv279_18329.txt',\n",
       " 'pos/cv280_8267.txt',\n",
       " 'pos/cv281_23253.txt',\n",
       " 'pos/cv282_6653.txt',\n",
       " 'pos/cv283_11055.txt',\n",
       " 'pos/cv284_19119.txt',\n",
       " 'pos/cv285_16494.txt',\n",
       " 'pos/cv286_25050.txt',\n",
       " 'pos/cv287_15900.txt',\n",
       " 'pos/cv288_18791.txt',\n",
       " 'pos/cv289_6463.txt',\n",
       " 'pos/cv290_11084.txt',\n",
       " 'pos/cv291_26635.txt',\n",
       " 'pos/cv292_7282.txt',\n",
       " 'pos/cv293_29856.txt',\n",
       " 'pos/cv294_11684.txt',\n",
       " 'pos/cv295_15570.txt',\n",
       " 'pos/cv296_12251.txt',\n",
       " 'pos/cv297_10047.txt',\n",
       " 'pos/cv298_23111.txt',\n",
       " 'pos/cv299_16214.txt',\n",
       " 'pos/cv300_22284.txt',\n",
       " 'pos/cv301_12146.txt',\n",
       " 'pos/cv302_25649.txt',\n",
       " 'pos/cv303_27520.txt',\n",
       " 'pos/cv304_28706.txt',\n",
       " 'pos/cv305_9946.txt',\n",
       " 'pos/cv306_10364.txt',\n",
       " 'pos/cv307_25270.txt',\n",
       " 'pos/cv308_5016.txt',\n",
       " 'pos/cv309_22571.txt',\n",
       " 'pos/cv310_13091.txt',\n",
       " 'pos/cv311_16002.txt',\n",
       " 'pos/cv312_29377.txt',\n",
       " 'pos/cv313_18198.txt',\n",
       " 'pos/cv314_14422.txt',\n",
       " 'pos/cv315_11629.txt',\n",
       " 'pos/cv316_6370.txt',\n",
       " 'pos/cv317_24049.txt',\n",
       " 'pos/cv318_10493.txt',\n",
       " 'pos/cv319_14727.txt',\n",
       " 'pos/cv320_9530.txt',\n",
       " 'pos/cv321_12843.txt',\n",
       " 'pos/cv322_20318.txt',\n",
       " 'pos/cv323_29805.txt',\n",
       " 'pos/cv324_7082.txt',\n",
       " 'pos/cv325_16629.txt',\n",
       " 'pos/cv326_13295.txt',\n",
       " 'pos/cv327_20292.txt',\n",
       " 'pos/cv328_10373.txt',\n",
       " 'pos/cv329_29370.txt',\n",
       " 'pos/cv330_29809.txt',\n",
       " 'pos/cv331_8273.txt',\n",
       " 'pos/cv332_16307.txt',\n",
       " 'pos/cv333_8916.txt',\n",
       " 'pos/cv334_10001.txt',\n",
       " 'pos/cv335_14665.txt',\n",
       " 'pos/cv336_10143.txt',\n",
       " 'pos/cv337_29181.txt',\n",
       " 'pos/cv338_8821.txt',\n",
       " 'pos/cv339_21119.txt',\n",
       " 'pos/cv340_13287.txt',\n",
       " 'pos/cv341_24430.txt',\n",
       " 'pos/cv342_19456.txt',\n",
       " 'pos/cv343_10368.txt',\n",
       " 'pos/cv344_5312.txt',\n",
       " 'pos/cv345_9954.txt',\n",
       " 'pos/cv346_18168.txt',\n",
       " 'pos/cv347_13194.txt',\n",
       " 'pos/cv348_18176.txt',\n",
       " 'pos/cv349_13507.txt',\n",
       " 'pos/cv350_20670.txt',\n",
       " 'pos/cv351_15458.txt',\n",
       " 'pos/cv352_5524.txt',\n",
       " 'pos/cv353_18159.txt',\n",
       " 'pos/cv354_8132.txt',\n",
       " 'pos/cv355_16413.txt',\n",
       " 'pos/cv356_25163.txt',\n",
       " 'pos/cv357_13156.txt',\n",
       " 'pos/cv358_10691.txt',\n",
       " 'pos/cv359_6647.txt',\n",
       " 'pos/cv360_8398.txt',\n",
       " 'pos/cv361_28944.txt',\n",
       " 'pos/cv362_15341.txt',\n",
       " 'pos/cv363_29332.txt',\n",
       " 'pos/cv364_12901.txt',\n",
       " 'pos/cv365_11576.txt',\n",
       " 'pos/cv366_10221.txt',\n",
       " 'pos/cv367_22792.txt',\n",
       " 'pos/cv368_10466.txt',\n",
       " 'pos/cv369_12886.txt',\n",
       " 'pos/cv370_5221.txt',\n",
       " 'pos/cv371_7630.txt',\n",
       " 'pos/cv372_6552.txt',\n",
       " 'pos/cv373_20404.txt',\n",
       " 'pos/cv374_25436.txt',\n",
       " 'pos/cv375_9929.txt',\n",
       " 'pos/cv376_19435.txt',\n",
       " 'pos/cv377_7946.txt',\n",
       " 'pos/cv378_20629.txt',\n",
       " 'pos/cv379_21963.txt',\n",
       " 'pos/cv380_7574.txt',\n",
       " 'pos/cv381_20172.txt',\n",
       " 'pos/cv382_7897.txt',\n",
       " 'pos/cv383_13116.txt',\n",
       " 'pos/cv384_17140.txt',\n",
       " 'pos/cv385_29741.txt',\n",
       " 'pos/cv386_10080.txt',\n",
       " 'pos/cv387_11507.txt',\n",
       " 'pos/cv388_12009.txt',\n",
       " 'pos/cv389_9369.txt',\n",
       " 'pos/cv390_11345.txt',\n",
       " 'pos/cv391_10802.txt',\n",
       " 'pos/cv392_11458.txt',\n",
       " 'pos/cv393_29327.txt',\n",
       " 'pos/cv394_5137.txt',\n",
       " 'pos/cv395_10849.txt',\n",
       " 'pos/cv396_17989.txt',\n",
       " 'pos/cv397_29023.txt',\n",
       " 'pos/cv398_15537.txt',\n",
       " 'pos/cv399_2877.txt',\n",
       " 'pos/cv400_19220.txt',\n",
       " 'pos/cv401_12605.txt',\n",
       " 'pos/cv402_14425.txt',\n",
       " 'pos/cv403_6621.txt',\n",
       " 'pos/cv404_20315.txt',\n",
       " 'pos/cv405_20399.txt',\n",
       " 'pos/cv406_21020.txt',\n",
       " 'pos/cv407_22637.txt',\n",
       " 'pos/cv408_5297.txt',\n",
       " 'pos/cv409_29786.txt',\n",
       " 'pos/cv410_24266.txt',\n",
       " 'pos/cv411_15007.txt',\n",
       " 'pos/cv412_24095.txt',\n",
       " 'pos/cv413_7398.txt',\n",
       " 'pos/cv414_10518.txt',\n",
       " 'pos/cv415_22517.txt',\n",
       " 'pos/cv416_11136.txt',\n",
       " 'pos/cv417_13115.txt',\n",
       " 'pos/cv418_14774.txt',\n",
       " 'pos/cv419_13394.txt',\n",
       " 'pos/cv420_28795.txt',\n",
       " 'pos/cv421_9709.txt',\n",
       " 'pos/cv422_9381.txt',\n",
       " 'pos/cv423_11155.txt',\n",
       " 'pos/cv424_8831.txt',\n",
       " 'pos/cv425_8250.txt',\n",
       " 'pos/cv426_10421.txt',\n",
       " 'pos/cv427_10825.txt',\n",
       " 'pos/cv428_11347.txt',\n",
       " 'pos/cv429_7439.txt',\n",
       " 'pos/cv430_17351.txt',\n",
       " 'pos/cv431_7085.txt',\n",
       " 'pos/cv432_14224.txt',\n",
       " 'pos/cv433_10144.txt',\n",
       " 'pos/cv434_5793.txt',\n",
       " 'pos/cv435_23110.txt',\n",
       " 'pos/cv436_19179.txt',\n",
       " 'pos/cv437_22849.txt',\n",
       " 'pos/cv438_8043.txt',\n",
       " 'pos/cv439_15970.txt',\n",
       " 'pos/cv440_15243.txt',\n",
       " 'pos/cv441_13711.txt',\n",
       " 'pos/cv442_13846.txt',\n",
       " 'pos/cv443_21118.txt',\n",
       " 'pos/cv444_9974.txt',\n",
       " 'pos/cv445_25882.txt',\n",
       " 'pos/cv446_11353.txt',\n",
       " 'pos/cv447_27332.txt',\n",
       " 'pos/cv448_14695.txt',\n",
       " 'pos/cv449_8785.txt',\n",
       " 'pos/cv450_7890.txt',\n",
       " 'pos/cv451_10690.txt',\n",
       " 'pos/cv452_5088.txt',\n",
       " 'pos/cv453_10379.txt',\n",
       " 'pos/cv454_2053.txt',\n",
       " 'pos/cv455_29000.txt',\n",
       " 'pos/cv456_18985.txt',\n",
       " 'pos/cv457_18453.txt',\n",
       " 'pos/cv458_8604.txt',\n",
       " 'pos/cv459_20319.txt',\n",
       " 'pos/cv460_10842.txt',\n",
       " 'pos/cv461_19600.txt',\n",
       " 'pos/cv462_19350.txt',\n",
       " 'pos/cv463_10343.txt',\n",
       " 'pos/cv464_15650.txt',\n",
       " 'pos/cv465_22431.txt',\n",
       " 'pos/cv466_18722.txt',\n",
       " 'pos/cv467_25773.txt',\n",
       " 'pos/cv468_15228.txt',\n",
       " 'pos/cv469_20630.txt',\n",
       " 'pos/cv470_15952.txt',\n",
       " 'pos/cv471_16858.txt',\n",
       " 'pos/cv472_29280.txt',\n",
       " 'pos/cv473_7367.txt',\n",
       " 'pos/cv474_10209.txt',\n",
       " 'pos/cv475_21692.txt',\n",
       " 'pos/cv476_16856.txt',\n",
       " 'pos/cv477_22479.txt',\n",
       " 'pos/cv478_14309.txt',\n",
       " 'pos/cv479_5649.txt',\n",
       " 'pos/cv480_19817.txt',\n",
       " 'pos/cv481_7436.txt',\n",
       " 'pos/cv482_10580.txt',\n",
       " 'pos/cv483_16378.txt',\n",
       " 'pos/cv484_25054.txt',\n",
       " 'pos/cv485_26649.txt',\n",
       " 'pos/cv486_9799.txt',\n",
       " 'pos/cv487_10446.txt',\n",
       " 'pos/cv488_19856.txt',\n",
       " 'pos/cv489_17906.txt',\n",
       " 'pos/cv490_17872.txt',\n",
       " 'pos/cv491_12145.txt',\n",
       " 'pos/cv492_18271.txt',\n",
       " 'pos/cv493_12839.txt',\n",
       " 'pos/cv494_17389.txt',\n",
       " 'pos/cv495_14518.txt',\n",
       " 'pos/cv496_10530.txt',\n",
       " 'pos/cv497_26980.txt',\n",
       " 'pos/cv498_8832.txt',\n",
       " 'pos/cv499_10658.txt',\n",
       " 'pos/cv500_10251.txt',\n",
       " 'pos/cv501_11657.txt',\n",
       " 'pos/cv502_10406.txt',\n",
       " 'pos/cv503_10558.txt',\n",
       " 'pos/cv504_29243.txt',\n",
       " 'pos/cv505_12090.txt',\n",
       " 'pos/cv506_15956.txt',\n",
       " 'pos/cv507_9220.txt',\n",
       " 'pos/cv508_16006.txt',\n",
       " 'pos/cv509_15888.txt',\n",
       " 'pos/cv510_23360.txt',\n",
       " 'pos/cv511_10132.txt',\n",
       " 'pos/cv512_15965.txt',\n",
       " 'pos/cv513_6923.txt',\n",
       " 'pos/cv514_11187.txt',\n",
       " 'pos/cv515_17069.txt',\n",
       " 'pos/cv516_11172.txt',\n",
       " 'pos/cv517_19219.txt',\n",
       " 'pos/cv518_13331.txt',\n",
       " 'pos/cv519_14661.txt',\n",
       " 'pos/cv520_12295.txt',\n",
       " 'pos/cv521_15828.txt',\n",
       " 'pos/cv522_5583.txt',\n",
       " 'pos/cv523_16615.txt',\n",
       " 'pos/cv524_23627.txt',\n",
       " 'pos/cv525_16122.txt',\n",
       " 'pos/cv526_12083.txt',\n",
       " 'pos/cv527_10123.txt',\n",
       " 'pos/cv528_10822.txt',\n",
       " 'pos/cv529_10420.txt',\n",
       " 'pos/cv530_16212.txt',\n",
       " 'pos/cv531_26486.txt',\n",
       " 'pos/cv532_6522.txt',\n",
       " 'pos/cv533_9821.txt',\n",
       " 'pos/cv534_14083.txt',\n",
       " 'pos/cv535_19728.txt',\n",
       " 'pos/cv536_27134.txt',\n",
       " 'pos/cv537_12370.txt',\n",
       " 'pos/cv538_28667.txt',\n",
       " 'pos/cv539_20347.txt',\n",
       " 'pos/cv540_3421.txt',\n",
       " 'pos/cv541_28835.txt',\n",
       " 'pos/cv542_18980.txt',\n",
       " 'pos/cv543_5045.txt',\n",
       " 'pos/cv544_5108.txt',\n",
       " 'pos/cv545_12014.txt',\n",
       " 'pos/cv546_11767.txt',\n",
       " 'pos/cv547_16324.txt',\n",
       " 'pos/cv548_17731.txt',\n",
       " 'pos/cv549_21443.txt',\n",
       " 'pos/cv550_22211.txt',\n",
       " 'pos/cv551_10565.txt',\n",
       " 'pos/cv552_10016.txt',\n",
       " 'pos/cv553_26915.txt',\n",
       " 'pos/cv554_13151.txt',\n",
       " 'pos/cv555_23922.txt',\n",
       " 'pos/cv556_14808.txt',\n",
       " 'pos/cv557_11449.txt',\n",
       " 'pos/cv558_29507.txt',\n",
       " 'pos/cv559_0050.txt',\n",
       " 'pos/cv560_17175.txt',\n",
       " 'pos/cv561_9201.txt',\n",
       " 'pos/cv562_10359.txt',\n",
       " 'pos/cv563_17257.txt',\n",
       " 'pos/cv564_11110.txt',\n",
       " 'pos/cv565_29572.txt',\n",
       " 'pos/cv566_8581.txt',\n",
       " 'pos/cv567_29611.txt',\n",
       " 'pos/cv568_15638.txt',\n",
       " 'pos/cv569_26381.txt',\n",
       " 'pos/cv570_29082.txt',\n",
       " 'pos/cv571_29366.txt',\n",
       " 'pos/cv572_18657.txt',\n",
       " 'pos/cv573_29525.txt',\n",
       " 'pos/cv574_22156.txt',\n",
       " 'pos/cv575_21150.txt',\n",
       " 'pos/cv576_14094.txt',\n",
       " 'pos/cv577_28549.txt',\n",
       " 'pos/cv578_15094.txt',\n",
       " 'pos/cv579_11605.txt',\n",
       " 'pos/cv580_14064.txt',\n",
       " 'pos/cv581_19381.txt',\n",
       " 'pos/cv582_6559.txt',\n",
       " 'pos/cv583_29692.txt',\n",
       " 'pos/cv584_29722.txt',\n",
       " 'pos/cv585_22496.txt',\n",
       " 'pos/cv586_7543.txt',\n",
       " 'pos/cv587_19162.txt',\n",
       " 'pos/cv588_13008.txt',\n",
       " 'pos/cv589_12064.txt',\n",
       " 'pos/cv590_19290.txt',\n",
       " 'pos/cv591_23640.txt',\n",
       " 'pos/cv592_22315.txt',\n",
       " 'pos/cv593_10987.txt',\n",
       " 'pos/cv594_11039.txt',\n",
       " 'pos/cv595_25335.txt',\n",
       " 'pos/cv596_28311.txt',\n",
       " 'pos/cv597_26360.txt',\n",
       " 'pos/cv598_16452.txt',\n",
       " 'pos/cv599_20988.txt',\n",
       " 'pos/cv600_23878.txt',\n",
       " 'pos/cv601_23453.txt',\n",
       " 'pos/cv602_8300.txt',\n",
       " 'pos/cv603_17694.txt',\n",
       " 'pos/cv604_2230.txt',\n",
       " 'pos/cv605_11800.txt',\n",
       " 'pos/cv606_15985.txt',\n",
       " 'pos/cv607_7717.txt',\n",
       " 'pos/cv608_23231.txt',\n",
       " 'pos/cv609_23877.txt',\n",
       " 'pos/cv610_2287.txt',\n",
       " 'pos/cv611_21120.txt',\n",
       " 'pos/cv612_5461.txt',\n",
       " 'pos/cv613_21796.txt',\n",
       " 'pos/cv614_10626.txt',\n",
       " 'pos/cv615_14182.txt',\n",
       " 'pos/cv616_29319.txt',\n",
       " 'pos/cv617_9322.txt',\n",
       " 'pos/cv618_8974.txt',\n",
       " 'pos/cv619_12462.txt',\n",
       " 'pos/cv620_24265.txt',\n",
       " 'pos/cv621_14368.txt',\n",
       " 'pos/cv622_8147.txt',\n",
       " 'pos/cv623_15356.txt',\n",
       " 'pos/cv624_10744.txt',\n",
       " 'pos/cv625_12440.txt',\n",
       " 'pos/cv626_7410.txt',\n",
       " 'pos/cv627_11620.txt',\n",
       " 'pos/cv628_19325.txt',\n",
       " 'pos/cv629_14909.txt',\n",
       " 'pos/cv630_10057.txt',\n",
       " 'pos/cv631_4967.txt',\n",
       " 'pos/cv632_9610.txt',\n",
       " 'pos/cv633_29837.txt',\n",
       " 'pos/cv634_11101.txt',\n",
       " 'pos/cv635_10022.txt',\n",
       " 'pos/cv636_15279.txt',\n",
       " 'pos/cv637_1250.txt',\n",
       " 'pos/cv638_2953.txt',\n",
       " 'pos/cv639_10308.txt',\n",
       " 'pos/cv640_5378.txt',\n",
       " 'pos/cv641_12349.txt',\n",
       " 'pos/cv642_29867.txt',\n",
       " 'pos/cv643_29349.txt',\n",
       " 'pos/cv644_17154.txt',\n",
       " 'pos/cv645_15668.txt',\n",
       " 'pos/cv646_15065.txt',\n",
       " 'pos/cv647_13691.txt',\n",
       " 'pos/cv648_15792.txt',\n",
       " 'pos/cv649_12735.txt',\n",
       " 'pos/cv650_14340.txt',\n",
       " 'pos/cv651_10492.txt',\n",
       " 'pos/cv652_13972.txt',\n",
       " 'pos/cv653_19583.txt',\n",
       " 'pos/cv654_18246.txt',\n",
       " 'pos/cv655_11154.txt',\n",
       " 'pos/cv656_24201.txt',\n",
       " 'pos/cv657_24513.txt',\n",
       " 'pos/cv658_10532.txt',\n",
       " 'pos/cv659_19944.txt',\n",
       " 'pos/cv660_21893.txt',\n",
       " 'pos/cv661_2450.txt',\n",
       " 'pos/cv662_13320.txt',\n",
       " 'pos/cv663_13019.txt',\n",
       " 'pos/cv664_4389.txt',\n",
       " 'pos/cv665_29538.txt',\n",
       " 'pos/cv666_18963.txt',\n",
       " 'pos/cv667_18467.txt',\n",
       " 'pos/cv668_17604.txt',\n",
       " 'pos/cv669_22995.txt',\n",
       " 'pos/cv670_25826.txt',\n",
       " 'pos/cv671_5054.txt',\n",
       " 'pos/cv672_28083.txt',\n",
       " 'pos/cv673_24714.txt',\n",
       " 'pos/cv674_10732.txt',\n",
       " 'pos/cv675_21588.txt',\n",
       " 'pos/cv676_21090.txt',\n",
       " 'pos/cv677_17715.txt',\n",
       " 'pos/cv678_13419.txt',\n",
       " 'pos/cv679_28559.txt',\n",
       " 'pos/cv680_10160.txt',\n",
       " 'pos/cv681_9692.txt',\n",
       " 'pos/cv682_16139.txt',\n",
       " 'pos/cv683_12167.txt',\n",
       " 'pos/cv684_11798.txt',\n",
       " 'pos/cv685_5947.txt',\n",
       " 'pos/cv686_13900.txt',\n",
       " 'pos/cv687_21100.txt',\n",
       " 'pos/cv688_7368.txt',\n",
       " 'pos/cv689_12587.txt',\n",
       " 'pos/cv690_5619.txt',\n",
       " 'pos/cv691_5043.txt',\n",
       " 'pos/cv692_15451.txt',\n",
       " 'pos/cv693_18063.txt',\n",
       " 'pos/cv694_4876.txt',\n",
       " 'pos/cv695_21108.txt',\n",
       " 'pos/cv696_29740.txt',\n",
       " 'pos/cv697_11162.txt',\n",
       " 'pos/cv698_15253.txt',\n",
       " 'pos/cv699_7223.txt',\n",
       " 'pos/cv700_21947.txt',\n",
       " 'pos/cv701_14252.txt',\n",
       " 'pos/cv702_11500.txt',\n",
       " 'pos/cv703_16143.txt',\n",
       " 'pos/cv704_15969.txt',\n",
       " 'pos/cv705_11059.txt',\n",
       " 'pos/cv706_24716.txt',\n",
       " 'pos/cv707_10678.txt',\n",
       " 'pos/cv708_28729.txt',\n",
       " 'pos/cv709_10529.txt',\n",
       " 'pos/cv710_22577.txt',\n",
       " 'pos/cv711_11665.txt',\n",
       " 'pos/cv712_22920.txt',\n",
       " 'pos/cv713_29155.txt',\n",
       " 'pos/cv714_18502.txt',\n",
       " 'pos/cv715_18179.txt',\n",
       " 'pos/cv716_10514.txt',\n",
       " 'pos/cv717_15953.txt',\n",
       " 'pos/cv718_11434.txt',\n",
       " 'pos/cv719_5713.txt',\n",
       " 'pos/cv720_5389.txt',\n",
       " 'pos/cv721_29121.txt',\n",
       " 'pos/cv722_7110.txt',\n",
       " 'pos/cv723_8648.txt',\n",
       " 'pos/cv724_13681.txt',\n",
       " 'pos/cv725_10103.txt',\n",
       " 'pos/cv726_4719.txt',\n",
       " 'pos/cv727_4978.txt',\n",
       " 'pos/cv728_16133.txt',\n",
       " 'pos/cv729_10154.txt',\n",
       " 'pos/cv730_10279.txt',\n",
       " 'pos/cv731_4136.txt',\n",
       " 'pos/cv732_12245.txt',\n",
       " 'pos/cv733_9839.txt',\n",
       " 'pos/cv734_21568.txt',\n",
       " 'pos/cv735_18801.txt',\n",
       " 'pos/cv736_23670.txt',\n",
       " 'pos/cv737_28907.txt',\n",
       " 'pos/cv738_10116.txt',\n",
       " 'pos/cv739_11209.txt',\n",
       " 'pos/cv740_12445.txt',\n",
       " 'pos/cv741_11890.txt',\n",
       " 'pos/cv742_7751.txt',\n",
       " 'pos/cv743_15449.txt',\n",
       " 'pos/cv744_10038.txt',\n",
       " 'pos/cv745_12773.txt',\n",
       " 'pos/cv746_10147.txt',\n",
       " 'pos/cv747_16556.txt',\n",
       " 'pos/cv748_12786.txt',\n",
       " 'pos/cv749_17765.txt',\n",
       " 'pos/cv750_10180.txt',\n",
       " 'pos/cv751_15719.txt',\n",
       " 'pos/cv752_24155.txt',\n",
       " 'pos/cv753_10875.txt',\n",
       " 'pos/cv754_7216.txt',\n",
       " 'pos/cv755_23616.txt',\n",
       " 'pos/cv756_22540.txt',\n",
       " 'pos/cv757_10189.txt',\n",
       " 'pos/cv758_9671.txt',\n",
       " 'pos/cv759_13522.txt',\n",
       " 'pos/cv760_8597.txt',\n",
       " 'pos/cv761_12620.txt',\n",
       " 'pos/cv762_13927.txt',\n",
       " 'pos/cv763_14729.txt',\n",
       " 'pos/cv764_11739.txt',\n",
       " 'pos/cv765_19037.txt',\n",
       " 'pos/cv766_7540.txt',\n",
       " 'pos/cv767_14062.txt',\n",
       " 'pos/cv768_11751.txt',\n",
       " 'pos/cv769_8123.txt',\n",
       " 'pos/cv770_10451.txt',\n",
       " 'pos/cv771_28665.txt',\n",
       " 'pos/cv772_12119.txt',\n",
       " 'pos/cv773_18817.txt',\n",
       " 'pos/cv774_13845.txt',\n",
       " 'pos/cv775_16237.txt',\n",
       " 'pos/cv776_20529.txt',\n",
       " 'pos/cv777_10094.txt',\n",
       " 'pos/cv778_17330.txt',\n",
       " 'pos/cv779_17881.txt',\n",
       " 'pos/cv780_7984.txt',\n",
       " 'pos/cv781_5262.txt',\n",
       " 'pos/cv782_19526.txt',\n",
       " 'pos/cv783_13227.txt',\n",
       " 'pos/cv784_14394.txt',\n",
       " 'pos/cv785_22600.txt',\n",
       " 'pos/cv786_22497.txt',\n",
       " 'pos/cv787_13743.txt',\n",
       " 'pos/cv788_25272.txt',\n",
       " 'pos/cv789_12136.txt',\n",
       " 'pos/cv790_14600.txt',\n",
       " 'pos/cv791_16302.txt',\n",
       " 'pos/cv792_3832.txt',\n",
       " 'pos/cv793_13650.txt',\n",
       " 'pos/cv794_15868.txt',\n",
       " 'pos/cv795_10122.txt',\n",
       " 'pos/cv796_15782.txt',\n",
       " 'pos/cv797_6957.txt',\n",
       " 'pos/cv798_23531.txt',\n",
       " 'pos/cv799_18543.txt',\n",
       " 'pos/cv800_12368.txt',\n",
       " 'pos/cv801_25228.txt',\n",
       " 'pos/cv802_28664.txt',\n",
       " 'pos/cv803_8207.txt',\n",
       " 'pos/cv804_10862.txt',\n",
       " 'pos/cv805_19601.txt',\n",
       " 'pos/cv806_8842.txt',\n",
       " 'pos/cv807_21740.txt',\n",
       " 'pos/cv808_12635.txt',\n",
       " 'pos/cv809_5009.txt',\n",
       " 'pos/cv810_12458.txt',\n",
       " 'pos/cv811_21386.txt',\n",
       " 'pos/cv812_17924.txt',\n",
       " 'pos/cv813_6534.txt',\n",
       " 'pos/cv814_18975.txt',\n",
       " 'pos/cv815_22456.txt',\n",
       " 'pos/cv816_13655.txt',\n",
       " 'pos/cv817_4041.txt',\n",
       " 'pos/cv818_10211.txt',\n",
       " 'pos/cv819_9364.txt',\n",
       " 'pos/cv820_22892.txt',\n",
       " 'pos/cv821_29364.txt',\n",
       " 'pos/cv822_20049.txt',\n",
       " 'pos/cv823_15569.txt',\n",
       " 'pos/cv824_8838.txt',\n",
       " 'pos/cv825_5063.txt',\n",
       " 'pos/cv826_11834.txt',\n",
       " 'pos/cv827_18331.txt',\n",
       " 'pos/cv828_19831.txt',\n",
       " 'pos/cv829_20289.txt',\n",
       " 'pos/cv830_6014.txt',\n",
       " 'pos/cv831_14689.txt',\n",
       " 'pos/cv832_23275.txt',\n",
       " 'pos/cv833_11053.txt',\n",
       " 'pos/cv834_22195.txt',\n",
       " 'pos/cv835_19159.txt',\n",
       " 'pos/cv836_12968.txt',\n",
       " 'pos/cv837_27325.txt',\n",
       " 'pos/cv838_24728.txt',\n",
       " 'pos/cv839_21467.txt',\n",
       " 'pos/cv840_16321.txt',\n",
       " 'pos/cv841_3967.txt',\n",
       " 'pos/cv842_5866.txt',\n",
       " 'pos/cv843_15544.txt',\n",
       " 'pos/cv844_12690.txt',\n",
       " 'pos/cv845_14290.txt',\n",
       " 'pos/cv846_29497.txt',\n",
       " 'pos/cv847_1941.txt',\n",
       " 'pos/cv848_10036.txt',\n",
       " 'pos/cv849_15729.txt',\n",
       " 'pos/cv850_16466.txt',\n",
       " 'pos/cv851_20469.txt',\n",
       " 'pos/cv852_27523.txt',\n",
       " 'pos/cv853_29233.txt',\n",
       " 'pos/cv854_17740.txt',\n",
       " 'pos/cv855_20661.txt',\n",
       " 'pos/cv856_29013.txt',\n",
       " 'pos/cv857_15958.txt',\n",
       " 'pos/cv858_18819.txt',\n",
       " 'pos/cv859_14107.txt',\n",
       " 'pos/cv860_13853.txt',\n",
       " 'pos/cv861_1198.txt',\n",
       " 'pos/cv862_14324.txt',\n",
       " 'pos/cv863_7424.txt',\n",
       " 'pos/cv864_3416.txt',\n",
       " 'pos/cv865_2895.txt',\n",
       " 'pos/cv866_29691.txt',\n",
       " 'pos/cv867_16661.txt',\n",
       " 'pos/cv868_11948.txt',\n",
       " 'pos/cv869_23611.txt',\n",
       " 'pos/cv870_16348.txt',\n",
       " 'pos/cv871_24888.txt',\n",
       " 'pos/cv872_12591.txt',\n",
       " 'pos/cv873_18636.txt',\n",
       " 'pos/cv874_11236.txt',\n",
       " 'pos/cv875_5754.txt',\n",
       " 'pos/cv876_9390.txt',\n",
       " 'pos/cv877_29274.txt',\n",
       " 'pos/cv878_15694.txt',\n",
       " 'pos/cv879_14903.txt',\n",
       " 'pos/cv880_29800.txt',\n",
       " 'pos/cv881_13254.txt',\n",
       " 'pos/cv882_10026.txt',\n",
       " 'pos/cv883_27751.txt',\n",
       " 'pos/cv884_13632.txt',\n",
       " 'pos/cv885_12318.txt',\n",
       " 'pos/cv886_18177.txt',\n",
       " 'pos/cv887_5126.txt',\n",
       " 'pos/cv888_24435.txt',\n",
       " 'pos/cv889_21430.txt',\n",
       " 'pos/cv890_3977.txt',\n",
       " 'pos/cv891_6385.txt',\n",
       " 'pos/cv892_17576.txt',\n",
       " 'pos/cv893_26269.txt',\n",
       " 'pos/cv894_2068.txt',\n",
       " 'pos/cv895_21022.txt',\n",
       " 'pos/cv896_16071.txt',\n",
       " 'pos/cv897_10837.txt',\n",
       " 'pos/cv898_14187.txt',\n",
       " 'pos/cv899_16014.txt',\n",
       " 'pos/cv900_10331.txt',\n",
       " 'pos/cv901_11017.txt',\n",
       " 'pos/cv902_12256.txt',\n",
       " 'pos/cv903_17822.txt',\n",
       " 'pos/cv904_24353.txt',\n",
       " 'pos/cv905_29114.txt',\n",
       " 'pos/cv906_11491.txt',\n",
       " 'pos/cv907_3541.txt',\n",
       " 'pos/cv908_16009.txt',\n",
       " 'pos/cv909_9960.txt',\n",
       " 'pos/cv910_20488.txt',\n",
       " 'pos/cv911_20260.txt',\n",
       " 'pos/cv912_5674.txt',\n",
       " 'pos/cv913_29252.txt',\n",
       " 'pos/cv914_28742.txt',\n",
       " 'pos/cv915_8841.txt',\n",
       " 'pos/cv916_15467.txt',\n",
       " 'pos/cv917_29715.txt',\n",
       " 'pos/cv918_2693.txt',\n",
       " 'pos/cv919_16380.txt',\n",
       " 'pos/cv920_29622.txt',\n",
       " 'pos/cv921_12747.txt',\n",
       " 'pos/cv922_10073.txt',\n",
       " 'pos/cv923_11051.txt',\n",
       " 'pos/cv924_29540.txt',\n",
       " 'pos/cv925_8969.txt',\n",
       " 'pos/cv926_17059.txt',\n",
       " 'pos/cv927_10681.txt',\n",
       " 'pos/cv928_9168.txt',\n",
       " 'pos/cv929_16908.txt',\n",
       " 'pos/cv930_13475.txt',\n",
       " 'pos/cv931_17563.txt',\n",
       " 'pos/cv932_13401.txt',\n",
       " 'pos/cv933_23776.txt',\n",
       " 'pos/cv934_19027.txt',\n",
       " 'pos/cv935_23841.txt',\n",
       " 'pos/cv936_15954.txt',\n",
       " 'pos/cv937_9811.txt',\n",
       " 'pos/cv938_10220.txt',\n",
       " 'pos/cv939_10583.txt',\n",
       " 'pos/cv940_17705.txt',\n",
       " 'pos/cv941_10246.txt',\n",
       " 'pos/cv942_17082.txt',\n",
       " 'pos/cv943_22488.txt',\n",
       " 'pos/cv944_13521.txt',\n",
       " 'pos/cv945_12160.txt',\n",
       " 'pos/cv946_18658.txt',\n",
       " 'pos/cv947_10601.txt',\n",
       " 'pos/cv948_24606.txt',\n",
       " 'pos/cv949_20112.txt',\n",
       " 'pos/cv950_12350.txt',\n",
       " 'pos/cv951_10926.txt',\n",
       " 'pos/cv952_25240.txt',\n",
       " 'pos/cv953_6836.txt',\n",
       " 'pos/cv954_18628.txt',\n",
       " 'pos/cv955_25001.txt',\n",
       " 'pos/cv956_11609.txt',\n",
       " 'pos/cv957_8737.txt',\n",
       " 'pos/cv958_12162.txt',\n",
       " 'pos/cv959_14611.txt',\n",
       " 'pos/cv960_29007.txt',\n",
       " 'pos/cv961_5682.txt',\n",
       " 'pos/cv962_9803.txt',\n",
       " 'pos/cv963_6895.txt',\n",
       " 'pos/cv964_6021.txt',\n",
       " 'pos/cv965_26071.txt',\n",
       " 'pos/cv966_28832.txt',\n",
       " 'pos/cv967_5788.txt',\n",
       " 'pos/cv968_24218.txt',\n",
       " 'pos/cv969_13250.txt',\n",
       " 'pos/cv970_18450.txt',\n",
       " 'pos/cv971_10874.txt',\n",
       " 'pos/cv972_26417.txt',\n",
       " 'pos/cv973_10066.txt',\n",
       " 'pos/cv974_22941.txt',\n",
       " 'pos/cv975_10981.txt',\n",
       " 'pos/cv976_10267.txt',\n",
       " 'pos/cv977_4938.txt',\n",
       " 'pos/cv978_20929.txt',\n",
       " 'pos/cv979_18921.txt',\n",
       " 'pos/cv980_10953.txt',\n",
       " 'pos/cv981_14989.txt',\n",
       " 'pos/cv982_21103.txt',\n",
       " 'pos/cv983_22928.txt',\n",
       " 'pos/cv984_12767.txt',\n",
       " 'pos/cv985_6359.txt',\n",
       " 'pos/cv986_13527.txt',\n",
       " 'pos/cv987_6965.txt',\n",
       " 'pos/cv988_18740.txt',\n",
       " 'pos/cv989_15824.txt',\n",
       " 'pos/cv990_11591.txt',\n",
       " 'pos/cv991_18645.txt',\n",
       " 'pos/cv992_11962.txt',\n",
       " 'pos/cv993_29737.txt',\n",
       " 'pos/cv994_12270.txt',\n",
       " 'pos/cv995_21821.txt',\n",
       " 'pos/cv996_11592.txt',\n",
       " 'pos/cv997_5046.txt',\n",
       " 'pos/cv998_14111.txt',\n",
       " 'pos/cv999_13106.txt']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(fileids=['pos/cv000_29590.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n"
     ]
    }
   ],
   "source": [
    "# Load the reviews from the corpus \n",
    "fileids_pos = movie_reviews.fileids('pos')\n",
    "fileids_neg = movie_reviews.fileids('neg')\n",
    "\n",
    "# Extract the features from the reviews\n",
    "features_pos = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "features_neg = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "\n",
    "# Define the train and test split (80% and 20%)\n",
    "threshold = 0.8\n",
    "num_pos = int(threshold * len(features_pos))\n",
    "num_neg = int(threshold * len(features_neg))\n",
    "\n",
    "# Create training and training datasets\n",
    "features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "features_test = features_pos[num_pos:] + features_neg[num_neg:]  \n",
    "\n",
    "# Print the number of datapoints used\n",
    "print('\\nNumber of training datapoints:', len(features_train))\n",
    "print('Number of test datapoints:', len(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'plot': True,\n",
       "  ':': True,\n",
       "  'two': True,\n",
       "  'teen': True,\n",
       "  'couples': True,\n",
       "  'go': True,\n",
       "  'to': True,\n",
       "  'a': True,\n",
       "  'church': True,\n",
       "  'party': True,\n",
       "  ',': True,\n",
       "  'drink': True,\n",
       "  'and': True,\n",
       "  'then': True,\n",
       "  'drive': True,\n",
       "  '.': True,\n",
       "  'they': True,\n",
       "  'get': True,\n",
       "  'into': True,\n",
       "  'an': True,\n",
       "  'accident': True,\n",
       "  'one': True,\n",
       "  'of': True,\n",
       "  'the': True,\n",
       "  'guys': True,\n",
       "  'dies': True,\n",
       "  'but': True,\n",
       "  'his': True,\n",
       "  'girlfriend': True,\n",
       "  'continues': True,\n",
       "  'see': True,\n",
       "  'him': True,\n",
       "  'in': True,\n",
       "  'her': True,\n",
       "  'life': True,\n",
       "  'has': True,\n",
       "  'nightmares': True,\n",
       "  'what': True,\n",
       "  \"'\": True,\n",
       "  's': True,\n",
       "  'deal': True,\n",
       "  '?': True,\n",
       "  'watch': True,\n",
       "  'movie': True,\n",
       "  '\"': True,\n",
       "  'sorta': True,\n",
       "  'find': True,\n",
       "  'out': True,\n",
       "  'critique': True,\n",
       "  'mind': True,\n",
       "  '-': True,\n",
       "  'fuck': True,\n",
       "  'for': True,\n",
       "  'generation': True,\n",
       "  'that': True,\n",
       "  'touches': True,\n",
       "  'on': True,\n",
       "  'very': True,\n",
       "  'cool': True,\n",
       "  'idea': True,\n",
       "  'presents': True,\n",
       "  'it': True,\n",
       "  'bad': True,\n",
       "  'package': True,\n",
       "  'which': True,\n",
       "  'is': True,\n",
       "  'makes': True,\n",
       "  'this': True,\n",
       "  'review': True,\n",
       "  'even': True,\n",
       "  'harder': True,\n",
       "  'write': True,\n",
       "  'since': True,\n",
       "  'i': True,\n",
       "  'generally': True,\n",
       "  'applaud': True,\n",
       "  'films': True,\n",
       "  'attempt': True,\n",
       "  'break': True,\n",
       "  'mold': True,\n",
       "  'mess': True,\n",
       "  'with': True,\n",
       "  'your': True,\n",
       "  'head': True,\n",
       "  'such': True,\n",
       "  '(': True,\n",
       "  'lost': True,\n",
       "  'highway': True,\n",
       "  '&': True,\n",
       "  'memento': True,\n",
       "  ')': True,\n",
       "  'there': True,\n",
       "  'are': True,\n",
       "  'good': True,\n",
       "  'ways': True,\n",
       "  'making': True,\n",
       "  'all': True,\n",
       "  'types': True,\n",
       "  'these': True,\n",
       "  'folks': True,\n",
       "  'just': True,\n",
       "  'didn': True,\n",
       "  't': True,\n",
       "  'snag': True,\n",
       "  'correctly': True,\n",
       "  'seem': True,\n",
       "  'have': True,\n",
       "  'taken': True,\n",
       "  'pretty': True,\n",
       "  'neat': True,\n",
       "  'concept': True,\n",
       "  'executed': True,\n",
       "  'terribly': True,\n",
       "  'so': True,\n",
       "  'problems': True,\n",
       "  'well': True,\n",
       "  'its': True,\n",
       "  'main': True,\n",
       "  'problem': True,\n",
       "  'simply': True,\n",
       "  'too': True,\n",
       "  'jumbled': True,\n",
       "  'starts': True,\n",
       "  'off': True,\n",
       "  'normal': True,\n",
       "  'downshifts': True,\n",
       "  'fantasy': True,\n",
       "  'world': True,\n",
       "  'you': True,\n",
       "  'as': True,\n",
       "  'audience': True,\n",
       "  'member': True,\n",
       "  'no': True,\n",
       "  'going': True,\n",
       "  'dreams': True,\n",
       "  'characters': True,\n",
       "  'coming': True,\n",
       "  'back': True,\n",
       "  'from': True,\n",
       "  'dead': True,\n",
       "  'others': True,\n",
       "  'who': True,\n",
       "  'look': True,\n",
       "  'like': True,\n",
       "  'strange': True,\n",
       "  'apparitions': True,\n",
       "  'disappearances': True,\n",
       "  'looooot': True,\n",
       "  'chase': True,\n",
       "  'scenes': True,\n",
       "  'tons': True,\n",
       "  'weird': True,\n",
       "  'things': True,\n",
       "  'happen': True,\n",
       "  'most': True,\n",
       "  'not': True,\n",
       "  'explained': True,\n",
       "  'now': True,\n",
       "  'personally': True,\n",
       "  'don': True,\n",
       "  'trying': True,\n",
       "  'unravel': True,\n",
       "  'film': True,\n",
       "  'every': True,\n",
       "  'when': True,\n",
       "  'does': True,\n",
       "  'give': True,\n",
       "  'me': True,\n",
       "  'same': True,\n",
       "  'clue': True,\n",
       "  'over': True,\n",
       "  'again': True,\n",
       "  'kind': True,\n",
       "  'fed': True,\n",
       "  'up': True,\n",
       "  'after': True,\n",
       "  'while': True,\n",
       "  'biggest': True,\n",
       "  'obviously': True,\n",
       "  'got': True,\n",
       "  'big': True,\n",
       "  'secret': True,\n",
       "  'hide': True,\n",
       "  'seems': True,\n",
       "  'want': True,\n",
       "  'completely': True,\n",
       "  'until': True,\n",
       "  'final': True,\n",
       "  'five': True,\n",
       "  'minutes': True,\n",
       "  'do': True,\n",
       "  'make': True,\n",
       "  'entertaining': True,\n",
       "  'thrilling': True,\n",
       "  'or': True,\n",
       "  'engaging': True,\n",
       "  'meantime': True,\n",
       "  'really': True,\n",
       "  'sad': True,\n",
       "  'part': True,\n",
       "  'arrow': True,\n",
       "  'both': True,\n",
       "  'dig': True,\n",
       "  'flicks': True,\n",
       "  'we': True,\n",
       "  'actually': True,\n",
       "  'figured': True,\n",
       "  'by': True,\n",
       "  'half': True,\n",
       "  'way': True,\n",
       "  'point': True,\n",
       "  'strangeness': True,\n",
       "  'did': True,\n",
       "  'start': True,\n",
       "  'little': True,\n",
       "  'bit': True,\n",
       "  'sense': True,\n",
       "  'still': True,\n",
       "  'more': True,\n",
       "  'guess': True,\n",
       "  'bottom': True,\n",
       "  'line': True,\n",
       "  'movies': True,\n",
       "  'should': True,\n",
       "  'always': True,\n",
       "  'sure': True,\n",
       "  'before': True,\n",
       "  'given': True,\n",
       "  'password': True,\n",
       "  'enter': True,\n",
       "  'understanding': True,\n",
       "  'mean': True,\n",
       "  'showing': True,\n",
       "  'melissa': True,\n",
       "  'sagemiller': True,\n",
       "  'running': True,\n",
       "  'away': True,\n",
       "  'visions': True,\n",
       "  'about': True,\n",
       "  '20': True,\n",
       "  'throughout': True,\n",
       "  'plain': True,\n",
       "  'lazy': True,\n",
       "  '!': True,\n",
       "  'okay': True,\n",
       "  'people': True,\n",
       "  'chasing': True,\n",
       "  'know': True,\n",
       "  'need': True,\n",
       "  'how': True,\n",
       "  'giving': True,\n",
       "  'us': True,\n",
       "  'different': True,\n",
       "  'offering': True,\n",
       "  'further': True,\n",
       "  'insight': True,\n",
       "  'down': True,\n",
       "  'apparently': True,\n",
       "  'studio': True,\n",
       "  'took': True,\n",
       "  'director': True,\n",
       "  'chopped': True,\n",
       "  'themselves': True,\n",
       "  'shows': True,\n",
       "  'might': True,\n",
       "  've': True,\n",
       "  'been': True,\n",
       "  'decent': True,\n",
       "  'here': True,\n",
       "  'somewhere': True,\n",
       "  'suits': True,\n",
       "  'decided': True,\n",
       "  'turning': True,\n",
       "  'music': True,\n",
       "  'video': True,\n",
       "  'edge': True,\n",
       "  'would': True,\n",
       "  'actors': True,\n",
       "  'although': True,\n",
       "  'wes': True,\n",
       "  'bentley': True,\n",
       "  'seemed': True,\n",
       "  'be': True,\n",
       "  'playing': True,\n",
       "  'exact': True,\n",
       "  'character': True,\n",
       "  'he': True,\n",
       "  'american': True,\n",
       "  'beauty': True,\n",
       "  'only': True,\n",
       "  'new': True,\n",
       "  'neighborhood': True,\n",
       "  'my': True,\n",
       "  'kudos': True,\n",
       "  'holds': True,\n",
       "  'own': True,\n",
       "  'entire': True,\n",
       "  'feeling': True,\n",
       "  'unraveling': True,\n",
       "  'overall': True,\n",
       "  'doesn': True,\n",
       "  'stick': True,\n",
       "  'because': True,\n",
       "  'entertain': True,\n",
       "  'confusing': True,\n",
       "  'rarely': True,\n",
       "  'excites': True,\n",
       "  'feels': True,\n",
       "  'redundant': True,\n",
       "  'runtime': True,\n",
       "  'despite': True,\n",
       "  'ending': True,\n",
       "  'explanation': True,\n",
       "  'craziness': True,\n",
       "  'came': True,\n",
       "  'oh': True,\n",
       "  'horror': True,\n",
       "  'slasher': True,\n",
       "  'flick': True,\n",
       "  'packaged': True,\n",
       "  'someone': True,\n",
       "  'assuming': True,\n",
       "  'genre': True,\n",
       "  'hot': True,\n",
       "  'kids': True,\n",
       "  'also': True,\n",
       "  'wrapped': True,\n",
       "  'production': True,\n",
       "  'years': True,\n",
       "  'ago': True,\n",
       "  'sitting': True,\n",
       "  'shelves': True,\n",
       "  'ever': True,\n",
       "  'whatever': True,\n",
       "  'skip': True,\n",
       "  'where': True,\n",
       "  'joblo': True,\n",
       "  'nightmare': True,\n",
       "  'elm': True,\n",
       "  'street': True,\n",
       "  '3': True,\n",
       "  '7': True,\n",
       "  '/': True,\n",
       "  '10': True,\n",
       "  'blair': True,\n",
       "  'witch': True,\n",
       "  '2': True,\n",
       "  'crow': True,\n",
       "  '9': True,\n",
       "  'salvation': True,\n",
       "  '4': True,\n",
       "  'stir': True,\n",
       "  'echoes': True,\n",
       "  '8': True},\n",
       " 'Negative')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 15 most informative words:\n",
      "1. outstanding\n",
      "2. insulting\n",
      "3. vulnerable\n",
      "4. ludicrous\n",
      "5. uninvolving\n",
      "6. astounding\n",
      "7. avoids\n",
      "8. fascination\n",
      "9. affecting\n",
      "10. animators\n",
      "11. anna\n",
      "12. darker\n",
      "13. seagal\n",
      "14. symbol\n",
      "15. idiotic\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier \n",
    "classifier = NaiveBayesClassifier.train(features_train)\n",
    "print('\\nAccuracy of the classifier:', nltk_accuracy(classifier, features_test))\n",
    "\n",
    "\n",
    "N = 15\n",
    "print('\\nTop ' + str(N) + ' most informative words:')\n",
    "for i, item in enumerate(classifier.most_informative_features()):\n",
    "    print(str(i+1) + '. ' + item[0])\n",
    "    if i == N - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie review predictions:\n",
      "\n",
      "Review: The costumes in this movie were great\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.59\n",
      "\n",
      "Review: I think the story was terrible and the characters were very weak\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.8\n",
      "\n",
      "Review: People say that the director of the movie is amazing\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.6\n",
      "\n",
      "Review: This is such an idiotic movie. I will not recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Test input movie reviews\n",
    "input_reviews = [\n",
    "    'The costumes in this movie were great', \n",
    "    'I think the story was terrible and the characters were very weak',\n",
    "    'People say that the director of the movie is amazing', \n",
    "    'This is such an idiotic movie. I will not recommend it to anyone.' \n",
    "]\n",
    "\n",
    "print(\"\\nMovie review predictions:\")\n",
    "for review in input_reviews:\n",
    "    print(\"\\nReview:\", review)\n",
    "\n",
    "    # Compute the probabilities\n",
    "    probabilities = classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "    # Pick the maximum value\n",
    "    predicted_sentiment = probabilities.max()\n",
    "\n",
    "    # Print outputs\n",
    "    print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "    print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelowanie tematyczne z wykorzystaniem ukrytej alokacji Dirichleta (Latent Dirichlet Allocation, LDA, LDiA)\n",
    "**Modelowanie tematyczne to proces identyfikowania wzorców w danych tekstowych, które odpowiadają tematowi**. Jeśli tekst zawiera wiele tematów, można użyć tej techniki do zidentyfikowania i oddzielenia tych tematów w tekście wejściowym. Robimy to, aby odkryć ukrytą strukturę tematyczną w danym zestawie dokumentów.\n",
    "\n",
    "Modelowanie tematyczne pomaga nam w optymalnym organizowaniu naszych dokumentów, które można następnie wykorzystać do analizy. Jedną rzeczą, na którą należy zwrócić uwagę w przypadku algorytmów modelowania tematycznego, jest to, że nie potrzebujemy żadnych oznaczonych danych. Jest to jak **uczenie się bez nadzoru**, w którym algorytm samodzielnie identyfikuje wzorce. Biorąc pod uwagę ogromną ilość danych tekstowych generowanych w Internecie, modelowanie tematyczne staje się bardzo ważne, ponieważ umożliwia podsumowanie wszystkich tych danych, co w innym przypadku nie byłoby możliwe.\n",
    "\n",
    "**Utajona alokacja Dirichleta** to technika modelowania tematycznego, w której podstawową intuicją jest to, że dany fragment tekstu jest połączeniem wielu tematów. Rozważmy następujące zdanie \n",
    "\n",
    "- \"Wizualizacja danych jest ważnym narzędziem w analizie finansowej\". \n",
    "\n",
    "To zdanie zawiera wiele tematów, takich jak \"dane\", \"wizualizacja\", \"finanse\" i tak dalej. Ta szczególna kombinacja pomaga nam zidentyfikować ten tekst w dużym dokumencie. W istocie jest to model statystyczny, który próbuje uchwycić tę ideę i stworzyć na jej podstawie model. \n",
    "\n",
    "Model ten zakłada, że dokumenty są generowane z losowego procesu na podstawie tych tematów. **Temat to po prostu podział na ustalony zbiór słów.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Processor function for tokenizing, removing stop words, and stemming\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Create lemmatizer object \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get the list of stop words \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "    # Remove the stop words \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    # Perform stemming on the tokenized words \n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "    \n",
    "#     # Perform stemming on the tokenized words \n",
    "#     tokens_lemmatized = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Roman empire expanded very rapidly and it was the biggest empire in the world for a long time.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data\n",
    "data = load_data('data.txt')\n",
    "print(len(data))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roman',\n",
       " 'empire',\n",
       " 'expanded',\n",
       " 'rapidly',\n",
       " 'biggest',\n",
       " 'empire',\n",
       " 'world',\n",
       " 'long',\n",
       " 'time']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list for sentence tokens\n",
    "tokens = [process(sentence) for sentence in data]\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary based on the sentence tokens \n",
    "dict_tokens = corpora.Dictionary(documents=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'empire'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a document-term matrix\n",
    "doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "doc_term_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 contributing words to each topic:\n",
      "\n",
      "Topic 0\n",
      "\"empire\" ==> 5.1%\n",
      "\"time\" ==> 3.7%\n",
      "\"people\" ==> 3.7%\n",
      "\"demarcation\" ==> 2.2%\n",
      "\"historical\" ==> 2.2%\n",
      "\n",
      "Topic 1\n",
      "\"europe\" ==> 2.4%\n",
      "\"set\" ==> 2.4%\n",
      "\"structure\" ==> 2.4%\n",
      "\"formulate\" ==> 2.4%\n",
      "\"mathematics\" ==> 2.4%\n"
     ]
    }
   ],
   "source": [
    "# Define the number of topics for the LDA model\n",
    "num_topics = 2\n",
    "\n",
    "# Generate the LDA model \n",
    "ldamodel = models.ldamodel.LdaModel(doc_term_mat,\n",
    "                                    num_topics=num_topics, \n",
    "                                    id2word=dict_tokens, \n",
    "                                    passes=30)\n",
    "\n",
    "num_words = 5\n",
    "print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
    "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "    print('\\nTopic', item[0])\n",
    "\n",
    "    # Print the contributing words along with their relative contributions \n",
    "    list_of_strings = item[1].split(' + ')\n",
    "    for text in list_of_strings:\n",
    "        weight = text.split('*')[0]\n",
    "        word = text.split('*')[1]\n",
    "        print(word, '==>', str(round(float(weight) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że dość dobrze rozdziela dwa tematy - matematykę i historię. Jeśli przyjrzysz się tekstowi, możesz sprawdzić, czy każde zdanie dotyczy matematyki lub historii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "W tym rozdziale poznaliśmy różne koncepcje leżące u podstaw przetwarzania języka naturalnego. Omówiliśmy tokenizację i sposób rozdzielania tekstu wejściowego na wiele tokenów. Dowiedzieliśmy się, jak zredukować słowa do ich podstawowych form, używając tematyki i lematyzacji. Zaimplementowaliśmy fragment tekstu, aby podzielić tekst wejściowy na porcje w oparciu o predefiniowane warunki.\n",
    "\n",
    "Omówiliśmy model Bag of Words i stworzyliśmy macierz terminów dokumentu dla tekstu wejściowego. Następnie nauczyliśmy się kategoryzować tekst za pomocą uczenia maszynowego. Skonstruowaliśmy identyfikator płci za pomocą heurystyki. Wykorzystaliśmy uczenie maszynowe do analizy nastrojów recenzji filmów. Omówiliśmy modelowanie tematów i wdrożyliśmy system identyfikacji tematów w danym dokumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
